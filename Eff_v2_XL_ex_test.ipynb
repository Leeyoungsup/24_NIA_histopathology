{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_model_summary as tms\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",6)\n",
    "print(f\"Device:\\t\\t{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':2,\n",
    "        'data_path':'../../data/NIA/BRNT/',\n",
    "        'inch':3,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas,path):\n",
    "        \n",
    "        self.args=parmas\n",
    "\n",
    "        self.path=path\n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        trans1 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        image=trans1(Image.open(self.path[index]).convert('RGB').resize((params['image_size'],params['image_size'])))\n",
    "        image = self.trans(image)\n",
    "        path=self.path[index]\n",
    "        return image,path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "\n",
    "\n",
    "\n",
    "image_list=glob(params['data_path']+'*.jpeg')\n",
    "\n",
    "train_dataset=CustomDataset(params,image_list)\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('tf_efficientnetv2_xl', pretrained=True)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(custom_model, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size, 2048, 1, 1)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n",
    "        \n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "            \n",
    "import transformers\n",
    "\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "model = custom_model(2,1280,Feature_Extractor)\n",
    "model = model.to(device)\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=params['lr'], momentum=0.9)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "checkpoint = torch.load(\"../../model/detail_classification/BRNT/Eff_v2_XL_SAM_114.pt\")\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 147584: 100%|██████████| 147583/147583 [3:40:03<00:00, 11.18it/s]  \n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "val=tqdm(dataloader)\n",
    "model.eval()\n",
    "count=0\n",
    "val_running_loss=0.0\n",
    "acc_loss=0\n",
    "C1_list=[]\n",
    "C2_list=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x,path in val:\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        predict = model(x).to(device)\n",
    "        if predict.softmax(dim=1).argmax(dim=1).item()==0:\n",
    "            source=path[0]\n",
    "            destination='../../temp/BRNT/class1/'+os.path.basename(source) \n",
    "            shutil.copyfile(source, destination)\n",
    "\n",
    "        else:\n",
    "            source=path[0]\n",
    "            destination='../../temp/BRNT/class2/'+os.path.basename(source) \n",
    "            shutil.copyfile(source, destination)\n",
    "        val.set_description(f\"Step: {count+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C1_list=glob('../../temp/BRNT/class1/*.jpeg')\n",
    "C2_list=glob('../../temp/BRNT/class2/*.jpeg')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30209/30209 [09:08<00:00, 55.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=30209 Selection=5237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 15142/117374 [04:06<27:43, 61.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/shutil.py:217\u001b[0m, in \u001b[0;36m_samefile\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49msamefile(src, dst)\n\u001b[1;32m    218\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/genericpath.py:101\u001b[0m, in \u001b[0;36msamefile\u001b[0;34m(f1, f2)\u001b[0m\n\u001b[1;32m    100\u001b[0m s1 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstat(f1)\n\u001b[0;32m--> 101\u001b[0m s2 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(f2)\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m samestat(s1, s2)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../temp/BRNT/class2_selection/NIA6_R_BRNT_BRCA-YN-00169-S-TP-01_39_20.jpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m         source\u001b[39m=\u001b[39mC2_list[i]\n\u001b[1;32m     22\u001b[0m         destination\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../../temp/BRNT/class2_selection/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(source) \n\u001b[0;32m---> 23\u001b[0m         shutil\u001b[39m.\u001b[39;49mcopyfile(source, destination)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtotal=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(C2_list)\u001b[39m}\u001b[39;00m\u001b[39m Selection=\u001b[39m\u001b[39m{\u001b[39;00mSelection_count\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/shutil.py:240\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy data from src to dst in the most efficient way possible.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[39mIf follow_symlinks is not set and src is a symbolic link, a new\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39msymlink will be created instead of copying the file it points to.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mshutil.copyfile\u001b[39m\u001b[39m\"\u001b[39m, src, dst)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mif\u001b[39;00m _samefile(src, dst):\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m SameFileError(\u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m are the same file\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(src, dst))\n\u001b[1;32m    243\u001b[0m file_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/shutil.py:217\u001b[0m, in \u001b[0;36m_samefile\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(os\u001b[39m.\u001b[39mpath, \u001b[39m'\u001b[39m\u001b[39msamefile\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49msamefile(src, dst)\n\u001b[1;32m    218\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "Selection_count=0\n",
    "for i in tqdm(range(len(C1_list))):\n",
    "    img=Image.open(C1_list[i])\n",
    "    np_img=np.array(img)\n",
    "    if len(np.where(np_img[:,:,1]>200)[0])<1024*1024/5:\n",
    "        Selection_count+=1\n",
    "        source=C1_list[i]\n",
    "        destination='../../temp/BRNT/class1_selection/'+os.path.basename(source) \n",
    "        shutil.copyfile(source, destination)\n",
    "    \n",
    "print(f'total={len(C1_list)} Selection={Selection_count}')\n",
    "\n",
    "Selection_count=0\n",
    "for i in tqdm(range(len(C2_list))):\n",
    "    img=Image.open(C2_list[i])\n",
    "    np_img=np.array(img)\n",
    "    if len(np.where(np_img[:,:,1]>200)[0])<1024*1024/5:\n",
    "        Selection_count+=1\n",
    "        source=C2_list[i]\n",
    "        destination='../../temp/BRNT/class2_selection/'+os.path.basename(source) \n",
    "        shutil.copyfile(source, destination)\n",
    "    \n",
    "print(f'total={len(C2_list)} Selection={Selection_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=30209 Selection=4664\n"
     ]
    }
   ],
   "source": [
    "print(f'total={len(C1_list)} Selection={Selection_count}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
