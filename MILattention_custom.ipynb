{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import csv\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import timm\n",
    "import random\n",
    "from torchinfo import summary\n",
    "from glob import glob\n",
    "from torchvision.transforms import ToTensor\n",
    "nltk.download('punkt')\n",
    "tf = ToTensor()\n",
    "# Device configurationresul\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':1024,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':8,\n",
    "        'epochs':10000,\n",
    "        'data_path':'../../data/synth/type/',\n",
    "        'train_csv':'BR_train.csv',\n",
    "        'val_csv':'BR_val.csv',\n",
    "        'vocab_path':'../../data/synth/type/BR_vocab.pkl',\n",
    "        'embed_size':300,\n",
    "        'hidden_size':256,\n",
    "        'num_layers':1,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,data_list, data_path,image_size, csv, class_dataset, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = data_path+'**/**/'\n",
    "        self.df = pd.read_csv(data_path+csv)\n",
    "        self.class_dataset=class_dataset\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.image_size=image_size\n",
    "        self.data_list=data_list\n",
    "        \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        df = self.df\n",
    "        vocab = self.vocab\n",
    "        img_id=df.loc[index]\n",
    "        \n",
    "        caption=img_id['caption']\n",
    "        images = self.trans(self.data_list[index])\n",
    "        # Convert caption (string) to word ids.\n",
    "        \n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return images, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "\n",
    "def idx2word(vocab, indices):\n",
    "    sentence = []\n",
    "    \n",
    "    aa=indices.cpu().numpy()\n",
    "    \n",
    "    for index in aa:\n",
    "        word = vocab.idx2word[index]\n",
    "        sentence.append(word)\n",
    "    return sentence\n",
    "def word2sentence(words_list):\n",
    "    sentence=''\n",
    "    for word in words_list:\n",
    "        if word.isalnum():\n",
    "            sentence+=' '+word\n",
    "        else:\n",
    "            sentence+=word\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('efficientnetv2_s')\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class AttentionMILModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(AttentionMILModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Flatten the inputs\n",
    "        inputs = inputs.view(-1, channels, height, width)\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size , 2048, 1, 1)\n",
    "        \n",
    "        # Reshape features\n",
    "        features = features.view(batch_size, -1)  # Shape: (batch_size, num_tiles, 2048)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits  \n",
    "    \n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_heads, hidden_size, num_layers, max_seq_length=100):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_size))\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_size)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions, lengths, teacher_forcing_ratio=0.5):\n",
    "        batch_size, seq_len = captions.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.linear.out_features).to(captions.device)\n",
    "        \n",
    "        # Positional encoding을 더해 임베딩 생성\n",
    "        captions_embedded = self.embed(captions) + self.positional_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # features의 차원을 (batch_size, 1, embed_size)로 맞춤\n",
    "        features = features.unsqueeze(1)\n",
    "        \n",
    "        # Transformer는 (seq_len, batch_size, embed_size)로 입력을 받으므로 차원 변경\n",
    "        memory = features.permute(1, 0, 2)  # (1, batch_size, embed_size)\n",
    "        \n",
    "        input_caption = captions[:, 0].unsqueeze(1)  # Start with the first token\n",
    "        for t in range(1, seq_len):\n",
    "            input_embedded = self.embed(input_caption) + self.positional_encoding[:, :input_caption.size(1), :]\n",
    "            input_embedded = input_embedded.permute(1, 0, 2)  # (seq_len, batch_size, embed_size)\n",
    "            \n",
    "            # Transformer Decoder에 입력\n",
    "            transformer_output = self.transformer_decoder(input_embedded, memory)\n",
    "            \n",
    "            # 다시 차원을 (batch_size, seq_len, embed_size)로 변경 후 Linear layer에 전달\n",
    "            transformer_output = transformer_output.permute(1, 0, 2)\n",
    "            output = self.linear(transformer_output[:, -1, :])  # (batch_size, vocab_size)\n",
    "            \n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            # Teacher forcing 결정\n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            top1 = output.argmax(1)\n",
    "            input_caption = captions[:, t].unsqueeze(1) if use_teacher_forcing else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, max_seq_length=None):\n",
    "        \"\"\"Greedy Search 방식으로 시퀀스를 샘플링합니다.\"\"\"\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = self.max_seq_length\n",
    "        \n",
    "        # 샘플링을 위한 기본 설정\n",
    "        inputs = features.unsqueeze(1)  # (batch_size, 1, embed_size)\n",
    "        sampled_ids = []\n",
    "        \n",
    "        # 첫 번째 토큰은 <start> 토큰으로 간주 (일반적으로 ID는 1로 설정)\n",
    "        input_tokens = torch.ones(features.size(0), 1).long().to(features.device)\n",
    "        \n",
    "        for _ in range(max_seq_length):\n",
    "            # 임베딩 및 positional encoding 적용\n",
    "            embedded_tokens = self.embed(input_tokens) + self.positional_encoding[:, :input_tokens.size(1), :]\n",
    "            \n",
    "            # Transformer는 (seq_len, batch_size, embed_size) 형태의 입력이 필요함\n",
    "            embedded_tokens = embedded_tokens.permute(1, 0, 2)\n",
    "            memory = features.unsqueeze(1).permute(1, 0, 2)\n",
    "            \n",
    "            # Transformer 디코더를 사용하여 출력 생성\n",
    "            transformer_output = self.transformer_decoder(embedded_tokens, memory)\n",
    "            transformer_output = transformer_output.permute(1, 0, 2)\n",
    "            \n",
    "            # Linear layer로 vocab 크기로 변환\n",
    "            output = self.linear(transformer_output[:, -1, :])  # (batch_size, vocab_size)\n",
    "            _, predicted = output.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            \n",
    "            # 예측된 단어를 다음 입력으로 사용\n",
    "            input_tokens = torch.cat([input_tokens, predicted.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        return sampled_ids\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [01:00<00:00, 27.10it/s]\n",
      "100%|██████████| 206/206 [00:07<00:00, 28.74it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(params['vocab_path'], 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "transform = transforms.Compose([ \n",
    "        transforms.RandomCrop(params['image_size']),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "df=pd.read_csv(params['data_path']+params['train_csv'])\n",
    "train_list=torch.zeros(len(df),3,params['image_size'],params['image_size'])\n",
    "for i in tqdm(range(len(df))):\n",
    "    image=transform(Image.open(glob(params['data_path']+'**/**/'+df.loc[i]['path'])[0]).resize((params['image_size'],params['image_size'])))\n",
    "    train_list[i]=image\n",
    "df=pd.read_csv(params['data_path']+params['val_csv'])\n",
    "test_list=torch.zeros(len(df),3,params['image_size'],params['image_size'])\n",
    "for i in tqdm(range(len(df))):\n",
    "    image=transform(Image.open(glob(params['data_path']+'**/**/'+df.loc[i]['path'])[0]).resize((params['image_size'],params['image_size'])))\n",
    "    test_list[i]=image\n",
    "train_dataset=CustomDataset(train_list,params['data_path'],params['image_size'],params['train_csv'],'train',vocab,transform=transform)\n",
    "test_dataset=CustomDataset(test_list,params['data_path'],params['image_size'],params['val_csv'],'val',vocab,transform=transform)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True,collate_fn=collate_fn)\n",
    "val_dataloader=DataLoader(test_dataset,batch_size=params['batch_size'],shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "encoder = AttentionMILModel(params['embed_size'], 1280, Feature_Extractor).to(device)\n",
    "decoder = DecoderTransformer(params['embed_size'], len(vocab), 15, params['hidden_size'], params['num_layers']).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_param = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_param, lr=params['lr'], betas=(params['beta1'], params['beta2']))\n",
    "# summary(encoder, input_size=(params['batch_size'], 3, params['image_size'], params['image_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch: 1/10000 Step: 208 loss : 2.7580 : 100%|██████████| 207/207 [01:23<00:00,  2.48it/s]\n",
      "val epoch: 1/10000 Step: 27 loss : 3.6710 : 100%|██████████| 26/26 [00:04<00:00,  5.97it/s]\n",
      "train epoch: 2/10000 Step: 208 loss : 2.0144 : 100%|██████████| 207/207 [01:22<00:00,  2.50it/s]\n",
      "val epoch: 2/10000 Step: 27 loss : 3.6341 : 100%|██████████| 26/26 [00:05<00:00,  4.83it/s]\n",
      "train epoch: 3/10000 Step: 208 loss : 1.9543 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 3/10000 Step: 27 loss : 3.1975 : 100%|██████████| 26/26 [00:04<00:00,  6.49it/s]\n",
      "train epoch: 4/10000 Step: 208 loss : 1.8834 : 100%|██████████| 207/207 [01:17<00:00,  2.66it/s]\n",
      "val epoch: 4/10000 Step: 27 loss : 2.8748 : 100%|██████████| 26/26 [00:04<00:00,  6.41it/s]\n",
      "train epoch: 5/10000 Step: 208 loss : 1.8887 : 100%|██████████| 207/207 [01:17<00:00,  2.66it/s]\n",
      "val epoch: 5/10000 Step: 27 loss : 2.8403 : 100%|██████████| 26/26 [00:03<00:00,  6.54it/s]\n",
      "train epoch: 6/10000 Step: 208 loss : 1.8662 : 100%|██████████| 207/207 [01:18<00:00,  2.65it/s]\n",
      "val epoch: 6/10000 Step: 27 loss : 3.0917 : 100%|██████████| 26/26 [00:04<00:00,  6.39it/s]\n",
      "train epoch: 7/10000 Step: 208 loss : 1.8702 : 100%|██████████| 207/207 [01:16<00:00,  2.70it/s]\n",
      "val epoch: 7/10000 Step: 27 loss : 2.6817 : 100%|██████████| 26/26 [00:04<00:00,  5.99it/s]\n",
      "train epoch: 8/10000 Step: 208 loss : 1.8797 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 8/10000 Step: 27 loss : 2.7450 : 100%|██████████| 26/26 [00:04<00:00,  6.11it/s]\n",
      "train epoch: 9/10000 Step: 208 loss : 1.9537 : 100%|██████████| 207/207 [01:17<00:00,  2.69it/s]\n",
      "val epoch: 9/10000 Step: 27 loss : 2.6360 : 100%|██████████| 26/26 [00:04<00:00,  6.46it/s]\n",
      "train epoch: 10/10000 Step: 208 loss : 1.8924 : 100%|██████████| 207/207 [01:17<00:00,  2.67it/s]\n",
      "val epoch: 10/10000 Step: 27 loss : 2.5076 : 100%|██████████| 26/26 [00:04<00:00,  6.19it/s]\n",
      "train epoch: 11/10000 Step: 208 loss : 1.9441 : 100%|██████████| 207/207 [01:20<00:00,  2.58it/s]\n",
      "val epoch: 11/10000 Step: 27 loss : 2.5131 : 100%|██████████| 26/26 [00:04<00:00,  6.30it/s]\n",
      "train epoch: 12/10000 Step: 208 loss : 1.8974 : 100%|██████████| 207/207 [01:16<00:00,  2.72it/s]\n",
      "val epoch: 12/10000 Step: 27 loss : 2.4799 : 100%|██████████| 26/26 [00:03<00:00,  6.52it/s]\n",
      "train epoch: 13/10000 Step: 208 loss : 1.8832 : 100%|██████████| 207/207 [01:16<00:00,  2.69it/s]\n",
      "val epoch: 13/10000 Step: 27 loss : 2.2552 : 100%|██████████| 26/26 [00:04<00:00,  6.31it/s]\n",
      "train epoch: 14/10000 Step: 208 loss : 1.9188 : 100%|██████████| 207/207 [01:22<00:00,  2.51it/s]\n",
      "val epoch: 14/10000 Step: 27 loss : 2.1315 : 100%|██████████| 26/26 [00:04<00:00,  6.20it/s]\n",
      "train epoch: 15/10000 Step: 208 loss : 1.9244 : 100%|██████████| 207/207 [01:18<00:00,  2.65it/s]\n",
      "val epoch: 15/10000 Step: 27 loss : 2.0840 : 100%|██████████| 26/26 [00:04<00:00,  6.24it/s]\n",
      "train epoch: 16/10000 Step: 208 loss : 1.8708 : 100%|██████████| 207/207 [01:17<00:00,  2.66it/s]\n",
      "val epoch: 16/10000 Step: 27 loss : 2.2628 : 100%|██████████| 26/26 [00:04<00:00,  6.39it/s]\n",
      "train epoch: 17/10000 Step: 208 loss : 1.9504 : 100%|██████████| 207/207 [01:17<00:00,  2.66it/s]\n",
      "val epoch: 17/10000 Step: 27 loss : 2.1879 : 100%|██████████| 26/26 [00:04<00:00,  6.15it/s]\n",
      "train epoch: 18/10000 Step: 208 loss : 1.9313 : 100%|██████████| 207/207 [01:18<00:00,  2.65it/s]\n",
      "val epoch: 18/10000 Step: 27 loss : 2.1243 : 100%|██████████| 26/26 [00:04<00:00,  6.30it/s]\n",
      "train epoch: 19/10000 Step: 208 loss : 1.8642 : 100%|██████████| 207/207 [01:17<00:00,  2.66it/s]\n",
      "val epoch: 19/10000 Step: 27 loss : 2.1554 : 100%|██████████| 26/26 [00:04<00:00,  6.09it/s]\n",
      "train epoch: 20/10000 Step: 208 loss : 1.8886 : 100%|██████████| 207/207 [01:21<00:00,  2.54it/s]\n",
      "val epoch: 20/10000 Step: 27 loss : 1.9800 : 100%|██████████| 26/26 [00:04<00:00,  6.30it/s]\n",
      "train epoch: 21/10000 Step: 208 loss : 1.8935 : 100%|██████████| 207/207 [01:18<00:00,  2.65it/s]\n",
      "val epoch: 21/10000 Step: 27 loss : 2.1070 : 100%|██████████| 26/26 [00:04<00:00,  6.16it/s]\n",
      "train epoch: 22/10000 Step: 208 loss : 1.9272 : 100%|██████████| 207/207 [01:20<00:00,  2.57it/s]\n",
      "val epoch: 22/10000 Step: 27 loss : 2.0006 : 100%|██████████| 26/26 [00:04<00:00,  6.50it/s]\n",
      "train epoch: 23/10000 Step: 208 loss : 1.8783 : 100%|██████████| 207/207 [01:18<00:00,  2.64it/s]\n",
      "val epoch: 23/10000 Step: 27 loss : 2.0219 : 100%|██████████| 26/26 [00:04<00:00,  6.11it/s]\n",
      "train epoch: 24/10000 Step: 208 loss : 1.8616 : 100%|██████████| 207/207 [01:20<00:00,  2.57it/s]\n",
      "val epoch: 24/10000 Step: 27 loss : 2.0604 : 100%|██████████| 26/26 [00:04<00:00,  6.25it/s]\n",
      "train epoch: 25/10000 Step: 208 loss : 1.8960 : 100%|██████████| 207/207 [01:17<00:00,  2.68it/s]\n",
      "val epoch: 25/10000 Step: 27 loss : 2.0361 : 100%|██████████| 26/26 [00:04<00:00,  6.40it/s]\n",
      "train epoch: 26/10000 Step: 208 loss : 1.8946 : 100%|██████████| 207/207 [01:17<00:00,  2.68it/s]\n",
      "val epoch: 26/10000 Step: 27 loss : 2.3723 : 100%|██████████| 26/26 [00:04<00:00,  6.26it/s]\n",
      "train epoch: 27/10000 Step: 208 loss : 1.8775 : 100%|██████████| 207/207 [01:18<00:00,  2.64it/s]\n",
      "val epoch: 27/10000 Step: 27 loss : 2.0429 : 100%|██████████| 26/26 [00:04<00:00,  6.13it/s]\n",
      "train epoch: 28/10000 Step: 208 loss : 1.8413 : 100%|██████████| 207/207 [01:18<00:00,  2.64it/s]\n",
      "val epoch: 28/10000 Step: 27 loss : 1.8478 : 100%|██████████| 26/26 [00:04<00:00,  6.18it/s]\n",
      "train epoch: 29/10000 Step: 208 loss : 1.8641 : 100%|██████████| 207/207 [01:18<00:00,  2.65it/s]\n",
      "val epoch: 29/10000 Step: 27 loss : 2.0662 : 100%|██████████| 26/26 [00:04<00:00,  6.29it/s]\n",
      "train epoch: 30/10000 Step: 208 loss : 1.8094 : 100%|██████████| 207/207 [01:20<00:00,  2.57it/s]\n",
      "val epoch: 30/10000 Step: 27 loss : 1.9133 : 100%|██████████| 26/26 [00:04<00:00,  5.26it/s]\n",
      "train epoch: 31/10000 Step: 208 loss : 1.8862 : 100%|██████████| 207/207 [01:23<00:00,  2.49it/s]\n",
      "val epoch: 31/10000 Step: 27 loss : 1.9778 : 100%|██████████| 26/26 [00:04<00:00,  6.27it/s]\n",
      "train epoch: 32/10000 Step: 208 loss : 1.8597 : 100%|██████████| 207/207 [01:21<00:00,  2.54it/s]\n",
      "val epoch: 32/10000 Step: 27 loss : 2.3937 : 100%|██████████| 26/26 [00:04<00:00,  5.98it/s]\n",
      "train epoch: 33/10000 Step: 208 loss : 1.8838 : 100%|██████████| 207/207 [01:21<00:00,  2.53it/s]\n",
      "val epoch: 33/10000 Step: 27 loss : 2.6493 : 100%|██████████| 26/26 [00:04<00:00,  6.23it/s]\n",
      "train epoch: 34/10000 Step: 208 loss : 1.8498 : 100%|██████████| 207/207 [01:18<00:00,  2.63it/s]\n",
      "val epoch: 34/10000 Step: 27 loss : 2.6403 : 100%|██████████| 26/26 [00:04<00:00,  6.18it/s]\n",
      "train epoch: 35/10000 Step: 208 loss : 1.8228 : 100%|██████████| 207/207 [01:18<00:00,  2.65it/s]\n",
      "val epoch: 35/10000 Step: 27 loss : 1.8295 : 100%|██████████| 26/26 [00:04<00:00,  6.20it/s]\n",
      "train epoch: 36/10000 Step: 208 loss : 1.7793 : 100%|██████████| 207/207 [01:19<00:00,  2.61it/s]\n",
      "val epoch: 36/10000 Step: 27 loss : 2.2205 : 100%|██████████| 26/26 [00:04<00:00,  6.22it/s]\n",
      "train epoch: 37/10000 Step: 208 loss : 1.8306 : 100%|██████████| 207/207 [01:20<00:00,  2.58it/s]\n",
      "val epoch: 37/10000 Step: 27 loss : 1.8300 : 100%|██████████| 26/26 [00:04<00:00,  6.16it/s]\n",
      "train epoch: 38/10000 Step: 208 loss : 1.7654 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 38/10000 Step: 27 loss : 2.0185 : 100%|██████████| 26/26 [00:04<00:00,  6.40it/s]\n",
      "train epoch: 39/10000 Step: 208 loss : 1.8119 : 100%|██████████| 207/207 [01:30<00:00,  2.29it/s]\n",
      "val epoch: 39/10000 Step: 27 loss : 2.3031 : 100%|██████████| 26/26 [00:04<00:00,  5.30it/s]\n",
      "train epoch: 40/10000 Step: 208 loss : 1.7869 : 100%|██████████| 207/207 [01:26<00:00,  2.39it/s]\n",
      "val epoch: 40/10000 Step: 27 loss : 1.9692 : 100%|██████████| 26/26 [00:06<00:00,  4.17it/s]\n",
      "train epoch: 41/10000 Step: 208 loss : 1.8325 : 100%|██████████| 207/207 [02:02<00:00,  1.69it/s]\n",
      "val epoch: 41/10000 Step: 27 loss : 1.8171 : 100%|██████████| 26/26 [00:05<00:00,  4.49it/s]\n",
      "train epoch: 42/10000 Step: 208 loss : 1.7280 : 100%|██████████| 207/207 [01:23<00:00,  2.49it/s]\n",
      "val epoch: 42/10000 Step: 27 loss : 3.2536 : 100%|██████████| 26/26 [00:04<00:00,  5.80it/s]\n",
      "train epoch: 43/10000 Step: 208 loss : 1.9516 : 100%|██████████| 207/207 [01:44<00:00,  1.98it/s]\n",
      "val epoch: 43/10000 Step: 27 loss : 1.9512 : 100%|██████████| 26/26 [00:05<00:00,  4.56it/s]\n",
      "train epoch: 44/10000 Step: 208 loss : 1.7660 : 100%|██████████| 207/207 [01:24<00:00,  2.46it/s]\n",
      "val epoch: 44/10000 Step: 27 loss : 1.8800 : 100%|██████████| 26/26 [00:04<00:00,  5.47it/s]\n",
      "train epoch: 45/10000 Step: 208 loss : 1.7863 : 100%|██████████| 207/207 [01:23<00:00,  2.47it/s]\n",
      "val epoch: 45/10000 Step: 27 loss : 2.0158 : 100%|██████████| 26/26 [00:04<00:00,  6.12it/s]\n",
      "train epoch: 46/10000 Step: 208 loss : 1.7370 : 100%|██████████| 207/207 [01:22<00:00,  2.50it/s]\n",
      "val epoch: 46/10000 Step: 27 loss : 1.9375 : 100%|██████████| 26/26 [00:04<00:00,  6.13it/s]\n",
      "train epoch: 47/10000 Step: 208 loss : 1.7316 : 100%|██████████| 207/207 [01:22<00:00,  2.52it/s]\n",
      "val epoch: 47/10000 Step: 27 loss : 1.8840 : 100%|██████████| 26/26 [00:04<00:00,  5.83it/s]\n",
      "train epoch: 48/10000 Step: 208 loss : 1.7582 : 100%|██████████| 207/207 [01:24<00:00,  2.46it/s]\n",
      "val epoch: 48/10000 Step: 27 loss : 1.9636 : 100%|██████████| 26/26 [00:04<00:00,  6.16it/s]\n",
      "train epoch: 49/10000 Step: 208 loss : 1.7555 : 100%|██████████| 207/207 [01:22<00:00,  2.51it/s]\n",
      "val epoch: 49/10000 Step: 27 loss : 1.8526 : 100%|██████████| 26/26 [00:04<00:00,  5.91it/s]\n",
      "train epoch: 50/10000 Step: 208 loss : 1.7633 : 100%|██████████| 207/207 [01:25<00:00,  2.43it/s]\n",
      "val epoch: 50/10000 Step: 27 loss : 1.8519 : 100%|██████████| 26/26 [00:04<00:00,  6.03it/s]\n",
      "train epoch: 51/10000 Step: 208 loss : 1.7156 : 100%|██████████| 207/207 [01:21<00:00,  2.53it/s]\n",
      "val epoch: 51/10000 Step: 27 loss : 1.7643 : 100%|██████████| 26/26 [00:04<00:00,  6.28it/s]\n",
      "train epoch: 52/10000 Step: 208 loss : 1.7167 : 100%|██████████| 207/207 [01:21<00:00,  2.55it/s]\n",
      "val epoch: 52/10000 Step: 27 loss : 2.0274 : 100%|██████████| 26/26 [00:04<00:00,  5.40it/s]\n",
      "train epoch: 53/10000 Step: 208 loss : 1.7319 : 100%|██████████| 207/207 [01:23<00:00,  2.49it/s]\n",
      "val epoch: 53/10000 Step: 27 loss : 1.7885 : 100%|██████████| 26/26 [00:04<00:00,  6.20it/s]\n",
      "train epoch: 54/10000 Step: 208 loss : 1.7084 : 100%|██████████| 207/207 [01:22<00:00,  2.51it/s]\n",
      "val epoch: 54/10000 Step: 27 loss : 1.8165 : 100%|██████████| 26/26 [00:04<00:00,  5.80it/s]\n",
      "train epoch: 55/10000 Step: 208 loss : 1.7553 : 100%|██████████| 207/207 [01:22<00:00,  2.51it/s]\n",
      "val epoch: 55/10000 Step: 27 loss : 2.2034 : 100%|██████████| 26/26 [00:04<00:00,  6.09it/s]\n",
      "train epoch: 56/10000 Step: 208 loss : 1.7738 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 56/10000 Step: 27 loss : 1.8439 : 100%|██████████| 26/26 [00:04<00:00,  5.89it/s]\n",
      "train epoch: 57/10000 Step: 208 loss : 1.7248 : 100%|██████████| 207/207 [01:18<00:00,  2.63it/s]\n",
      "val epoch: 57/10000 Step: 27 loss : 1.8633 : 100%|██████████| 26/26 [00:04<00:00,  6.32it/s]\n",
      "train epoch: 58/10000 Step: 208 loss : 1.6737 : 100%|██████████| 207/207 [01:18<00:00,  2.65it/s]\n",
      "val epoch: 58/10000 Step: 27 loss : 1.7922 : 100%|██████████| 26/26 [00:04<00:00,  6.22it/s]\n",
      "train epoch: 59/10000 Step: 208 loss : 1.7254 : 100%|██████████| 207/207 [01:18<00:00,  2.64it/s]\n",
      "val epoch: 59/10000 Step: 27 loss : 2.2351 : 100%|██████████| 26/26 [00:04<00:00,  6.14it/s]\n",
      "train epoch: 60/10000 Step: 208 loss : 1.7883 : 100%|██████████| 207/207 [01:19<00:00,  2.62it/s]\n",
      "val epoch: 60/10000 Step: 27 loss : 1.9051 : 100%|██████████| 26/26 [00:04<00:00,  5.40it/s]\n",
      "train epoch: 61/10000 Step: 208 loss : 1.6743 : 100%|██████████| 207/207 [01:20<00:00,  2.56it/s]\n",
      "val epoch: 61/10000 Step: 27 loss : 2.0785 : 100%|██████████| 26/26 [00:04<00:00,  6.35it/s]\n",
      "train epoch: 62/10000 Step: 208 loss : 1.7080 : 100%|██████████| 207/207 [01:17<00:00,  2.66it/s]\n",
      "val epoch: 62/10000 Step: 27 loss : 1.9972 : 100%|██████████| 26/26 [00:04<00:00,  6.39it/s]\n",
      "train epoch: 63/10000 Step: 208 loss : 1.6974 : 100%|██████████| 207/207 [01:20<00:00,  2.57it/s]\n",
      "val epoch: 63/10000 Step: 27 loss : 1.9030 : 100%|██████████| 26/26 [00:04<00:00,  6.06it/s]\n",
      "train epoch: 64/10000 Step: 208 loss : 1.6743 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 64/10000 Step: 27 loss : 1.8737 : 100%|██████████| 26/26 [00:04<00:00,  5.64it/s]\n",
      "train epoch: 65/10000 Step: 208 loss : 1.6832 : 100%|██████████| 207/207 [01:21<00:00,  2.53it/s]\n",
      "val epoch: 65/10000 Step: 27 loss : 1.9040 : 100%|██████████| 26/26 [00:04<00:00,  6.12it/s]\n",
      "train epoch: 66/10000 Step: 208 loss : 1.6969 : 100%|██████████| 207/207 [01:22<00:00,  2.51it/s]\n",
      "val epoch: 66/10000 Step: 27 loss : 1.9046 : 100%|██████████| 26/26 [00:04<00:00,  5.98it/s]\n",
      "train epoch: 67/10000 Step: 208 loss : 1.6428 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 67/10000 Step: 27 loss : 1.7110 : 100%|██████████| 26/26 [00:04<00:00,  6.19it/s]\n",
      "train epoch: 68/10000 Step: 208 loss : 1.6417 : 100%|██████████| 207/207 [01:22<00:00,  2.51it/s]\n",
      "val epoch: 68/10000 Step: 27 loss : 1.7524 : 100%|██████████| 26/26 [00:03<00:00,  6.61it/s]\n",
      "train epoch: 69/10000 Step: 208 loss : 1.7503 : 100%|██████████| 207/207 [01:18<00:00,  2.64it/s]\n",
      "val epoch: 69/10000 Step: 27 loss : 1.8903 : 100%|██████████| 26/26 [00:04<00:00,  5.58it/s]\n",
      "train epoch: 70/10000 Step: 208 loss : 1.6468 : 100%|██████████| 207/207 [01:21<00:00,  2.53it/s]\n",
      "val epoch: 70/10000 Step: 27 loss : 1.8923 : 100%|██████████| 26/26 [00:04<00:00,  6.30it/s]\n",
      "train epoch: 71/10000 Step: 208 loss : 1.6968 : 100%|██████████| 207/207 [01:17<00:00,  2.66it/s]\n",
      "val epoch: 71/10000 Step: 27 loss : 1.7981 : 100%|██████████| 26/26 [00:04<00:00,  6.34it/s]\n",
      "train epoch: 72/10000 Step: 208 loss : 1.6959 : 100%|██████████| 207/207 [01:21<00:00,  2.53it/s]\n",
      "val epoch: 72/10000 Step: 27 loss : 1.7637 : 100%|██████████| 26/26 [00:04<00:00,  5.56it/s]\n",
      "train epoch: 73/10000 Step: 208 loss : 1.6838 : 100%|██████████| 207/207 [01:24<00:00,  2.46it/s]\n",
      "val epoch: 73/10000 Step: 27 loss : 2.3065 : 100%|██████████| 26/26 [00:04<00:00,  5.57it/s]\n",
      "train epoch: 74/10000 Step: 208 loss : 1.6808 : 100%|██████████| 207/207 [01:22<00:00,  2.51it/s]\n",
      "val epoch: 74/10000 Step: 27 loss : 1.8074 : 100%|██████████| 26/26 [00:04<00:00,  6.07it/s]\n",
      "train epoch: 75/10000 Step: 208 loss : 1.6441 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 75/10000 Step: 27 loss : 1.9365 : 100%|██████████| 26/26 [00:04<00:00,  6.13it/s]\n",
      "train epoch: 76/10000 Step: 208 loss : 1.6598 : 100%|██████████| 207/207 [01:19<00:00,  2.62it/s]\n",
      "val epoch: 76/10000 Step: 27 loss : 1.7703 : 100%|██████████| 26/26 [00:04<00:00,  6.07it/s]\n",
      "train epoch: 77/10000 Step: 208 loss : 1.6431 : 100%|██████████| 207/207 [01:22<00:00,  2.50it/s]\n",
      "val epoch: 77/10000 Step: 27 loss : 1.8405 : 100%|██████████| 26/26 [00:04<00:00,  5.41it/s]\n",
      "train epoch: 78/10000 Step: 208 loss : 1.6790 : 100%|██████████| 207/207 [01:23<00:00,  2.49it/s]\n",
      "val epoch: 78/10000 Step: 27 loss : 2.2565 : 100%|██████████| 26/26 [00:04<00:00,  5.95it/s]\n",
      "train epoch: 79/10000 Step: 208 loss : 1.7184 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 79/10000 Step: 27 loss : 1.7184 : 100%|██████████| 26/26 [00:04<00:00,  5.89it/s]\n",
      "train epoch: 80/10000 Step: 208 loss : 1.6775 : 100%|██████████| 207/207 [01:18<00:00,  2.62it/s]\n",
      "val epoch: 80/10000 Step: 27 loss : 2.0528 : 100%|██████████| 26/26 [00:04<00:00,  6.05it/s]\n",
      "train epoch: 81/10000 Step: 208 loss : 1.6712 : 100%|██████████| 207/207 [01:21<00:00,  2.54it/s]\n",
      "val epoch: 81/10000 Step: 27 loss : 1.7978 : 100%|██████████| 26/26 [00:04<00:00,  5.67it/s]\n",
      "train epoch: 82/10000 Step: 208 loss : 1.6654 : 100%|██████████| 207/207 [01:20<00:00,  2.56it/s]\n",
      "val epoch: 82/10000 Step: 27 loss : 1.8953 : 100%|██████████| 26/26 [00:04<00:00,  6.21it/s]\n",
      "train epoch: 83/10000 Step: 208 loss : 1.6351 : 100%|██████████| 207/207 [01:21<00:00,  2.53it/s]\n",
      "val epoch: 83/10000 Step: 27 loss : 1.7608 : 100%|██████████| 26/26 [00:04<00:00,  5.27it/s]\n",
      "train epoch: 84/10000 Step: 208 loss : 1.6183 : 100%|██████████| 207/207 [01:20<00:00,  2.57it/s]\n",
      "val epoch: 84/10000 Step: 27 loss : 1.7716 : 100%|██████████| 26/26 [00:04<00:00,  6.33it/s]\n",
      "train epoch: 85/10000 Step: 208 loss : 1.6500 : 100%|██████████| 207/207 [01:21<00:00,  2.54it/s]\n",
      "val epoch: 85/10000 Step: 27 loss : 1.8992 : 100%|██████████| 26/26 [00:04<00:00,  5.75it/s]\n",
      "train epoch: 86/10000 Step: 208 loss : 1.7042 : 100%|██████████| 207/207 [01:23<00:00,  2.48it/s]\n",
      "val epoch: 86/10000 Step: 27 loss : 1.8967 : 100%|██████████| 26/26 [00:04<00:00,  6.17it/s]\n",
      "train epoch: 87/10000 Step: 208 loss : 1.6100 : 100%|██████████| 207/207 [01:20<00:00,  2.57it/s]\n",
      "val epoch: 87/10000 Step: 27 loss : 1.7515 : 100%|██████████| 26/26 [00:04<00:00,  6.39it/s]\n",
      "train epoch: 88/10000 Step: 208 loss : 1.6293 : 100%|██████████| 207/207 [01:19<00:00,  2.60it/s]\n",
      "val epoch: 88/10000 Step: 27 loss : 1.8488 : 100%|██████████| 26/26 [00:04<00:00,  6.47it/s]\n",
      "train epoch: 89/10000 Step: 208 loss : 1.5983 : 100%|██████████| 207/207 [01:19<00:00,  2.59it/s]\n",
      "val epoch: 89/10000 Step: 27 loss : 1.8289 : 100%|██████████| 26/26 [00:04<00:00,  6.12it/s]\n",
      "train epoch: 90/10000 Step: 208 loss : 1.5868 : 100%|██████████| 207/207 [01:21<00:00,  2.55it/s]\n",
      "val epoch: 90/10000 Step: 27 loss : 1.7660 : 100%|██████████| 26/26 [00:04<00:00,  6.16it/s]\n",
      "train epoch: 91/10000 Step: 208 loss : 1.5967 : 100%|██████████| 207/207 [01:17<00:00,  2.67it/s]\n",
      "val epoch: 91/10000 Step: 27 loss : 1.9150 : 100%|██████████| 26/26 [00:04<00:00,  6.30it/s]\n",
      "train epoch: 92/10000 Step: 208 loss : 1.6260 : 100%|██████████| 207/207 [01:17<00:00,  2.69it/s]\n",
      "val epoch: 92/10000 Step: 27 loss : 1.8727 : 100%|██████████| 26/26 [00:04<00:00,  6.29it/s]\n",
      "train epoch: 93/10000 Step: 206 loss : 1.6049 :  99%|█████████▉| 205/207 [01:17<00:00,  2.66it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "plt_count=0\n",
    "sum_loss= 1000.0\n",
    "scheduler = 0.90\n",
    "teacher_forcing=0.3\n",
    "for epoch in range(params['epochs']):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    train_loss = 0.0\n",
    "    for images,captions,lengths in train:\n",
    "        count+=1\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths, teacher_forcing_ratio=teacher_forcing*(scheduler**epoch))\n",
    "        outputs = pack_padded_sequence(outputs, lengths, batch_first=True)[0]\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "        train.set_description(f\"train epoch: {epoch+1}/{params['epochs']} Step: {count+1} loss : {train_loss/count:.4f} \")\n",
    "    with torch.no_grad():\n",
    "        val_count=0\n",
    "        val_loss = 0.0 \n",
    "        val_bleu_loss=0.0\n",
    "        val=tqdm(val_dataloader)\n",
    "        for images,captions,lengths in val:\n",
    "            val_count+=1\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths, teacher_forcing_ratio=0.0)\n",
    "            outputs = pack_padded_sequence(outputs, lengths, batch_first=True)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss+=loss.item()\n",
    "            val.set_description(f\"val epoch: {epoch+1}/{params['epochs']} Step: {val_count+1} loss : {(val_loss/val_count):.4f} \")\n",
    "    if val_loss<sum_loss:\n",
    "        sum_loss=val_loss\n",
    "        torch.save(encoder.state_dict(), '../../model/captioning/BR_encoder_check.pth')\n",
    "        torch.save(decoder.state_dict(), '../../model/captioning/BR_decoder_check.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
