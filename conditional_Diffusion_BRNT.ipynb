{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import Unet\n",
    "from conditionDiffusion.embedding import ConditionalEmbedding\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",1)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "tf=transforms.ToTensor()\n",
    "import pytorch_model_summary as tms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list=['유형8','유형9']\n",
    "params={'image_size':1024,\n",
    "        'lr':2e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'data_path':'../../data/normalization_type/BRIL/',\n",
    "        'image_count':5000,\n",
    "        'inch':1,\n",
    "        'modch': 128,\n",
    "        'outch': 1,\n",
    "        'chmul': [1, 2,2, 4,4, 8],\n",
    "        'numres':2,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':10,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':1,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        image = self.trans(image)\n",
    "        return image,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list=glob(params['data_path']+class_list[i]+'/*.jpeg')\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "        image_label.append(i)\n",
    "        \n",
    "train_images=torch.zeros((10,params['inch'],params['image_size'],params['image_size']))\n",
    "for i in tqdm(range(10)):\n",
    "    train_images[i]=tf(Image.open(image_path[i]).convert('L').resize((params['image_size'],params['image_size'])))*2-1\n",
    "train_dataset=CustomDataset(params,train_images,image_label)\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Unet(in_ch = params['inch'],\n",
    "            mod_ch = params['modch'],\n",
    "            out_ch = params['outch'],\n",
    "            ch_mul = params['chmul'],\n",
    "            num_res_blocks = params['numres'],\n",
    "            cdim = params['cdim'],\n",
    "            use_conv = params['useconv'],\n",
    "            droprate = params['droprate'],\n",
    "            dtype = params['dtype']\n",
    "            ).to(device)\n",
    "cemblayer = ConditionalEmbedding(len(class_list), params['cdim'], params['cdim']).to(device)\n",
    "betas = get_named_beta_schedule(num_diffusion_timesteps = params['T'])\n",
    "diffusion = GaussianDiffusion(\n",
    "                    dtype = params['dtype'],\n",
    "                    model = net,\n",
    "                    betas = betas,\n",
    "                    w = params['w'],\n",
    "                    v = params['v'],\n",
    "                    device = device\n",
    "                )\n",
    "optimizer = torch.optim.AdamW(\n",
    "                itertools.chain(\n",
    "                    diffusion.model.parameters(),\n",
    "                    cemblayer.parameters()\n",
    "                ),\n",
    "                lr = params['lr'],\n",
    "                weight_decay = 1e-6\n",
    "            )\n",
    "\n",
    "\n",
    "cosineScheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "warmUpScheduler = GradualWarmupScheduler(\n",
    "                        optimizer = optimizer,\n",
    "                        multiplier = params['multiplier'],\n",
    "                        warm_epoch = 50,\n",
    "                        after_scheduler = cosineScheduler,\n",
    "                        last_epoch = 0\n",
    "                    )\n",
    "# checkpoint=torch.load(f'../../model/conditionDiff/BR/ckpt_35_checkpoint.pt',map_location=device)\n",
    "# diffusion.model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "checkpoint=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "for epc in range(params['epochs']):\n",
    "    diffusion.model.train()\n",
    "    cemblayer.train()\n",
    "    total_loss=0\n",
    "    steps=0\n",
    "    with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, lab in tqdmDataLoader:\n",
    "            b = img.shape[0]\n",
    "            \n",
    "            x_0 = img.to(device)\n",
    "            lab = lab.to(device)\n",
    "            cemb = cemblayer(lab)\n",
    "            cemb[np.where(np.random.rand(b)<params['threshold'])] = 0\n",
    "            loss = diffusion.trainloss(x_0, cemb = cemb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            steps+=1\n",
    "            total_loss+=loss.item()\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"loss: \": total_loss/steps,\n",
    "                    \"batch per device: \":x_0.shape[0],\n",
    "                    \"img shape: \": x_0.shape[1:],\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                }\n",
    "            )\n",
    "    warmUpScheduler.step()\n",
    "    \n",
    "    diffusion.model.eval()\n",
    "    cemblayer.eval()\n",
    "    # generating samples\n",
    "    # The model generate 80 pictures(8 per row) each time\n",
    "    # pictures of same row belong to the same class\n",
    "    all_samples = []\n",
    "    each_device_batch =len(class_list)\n",
    "    with torch.no_grad():\n",
    "        lab = torch.ones(len(class_list), each_device_batch // len(class_list)).type(torch.long) \\\n",
    "        * torch.arange(start = 0, end = len(class_list)).reshape(-1, 1)\n",
    "        lab = lab.reshape(-1, 1).squeeze()\n",
    "        lab = lab.to(device)\n",
    "        cemb = cemblayer(lab)\n",
    "        genshape = (each_device_batch , 3, params['image_size'], params['image_size'])\n",
    "        if params['ddim']:\n",
    "            generated = diffusion.ddim_sample(\n",
    "                genshape, 100, 0.5, 'quadratic', cemb=cemb)\n",
    "        else:\n",
    "            generated = diffusion.sample(genshape, cemb = cemb)\n",
    "        generated=transback(generated)\n",
    "        for i in range(len(lab)):\n",
    "            img_pil = topilimage(generated[i].cpu())\n",
    "            img_pil.save(f'../../result/Detail/BRIL/{class_list[lab[i]]}/{epc}.png')\n",
    "\n",
    "        # save checkpoints\n",
    "        checkpoint = {\n",
    "                            'net':diffusion.model.state_dict(),\n",
    "                            'cemblayer':cemblayer.state_dict(),\n",
    "                            'optimizer':optimizer.state_dict(),\n",
    "                            'scheduler':warmUpScheduler.state_dict()\n",
    "                        }\n",
    "    torch.save(checkpoint, f'../../model/conditionDiff/details/BRIL/ckpt_{epc+1}_checkpoint.pt')\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        # 너비와 높이가 2배씩 감소\n",
    "        layers = [nn.Conv2d(in_channels, out_channels,\n",
    "                            kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# U-Net 아키텍처의 업 샘플링(Up Sampling) 모듈: Skip Connection 입력 사용\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        # 너비와 높이가 2배씩 증가\n",
    "        layers = [nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)  # 채널 레벨에서 합치기(concatenation)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# U-Net 생성자(Generator) 아키텍처\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        # 출력: [64 x 512 x 512]\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        # 출력: [128 x 256 x 256]\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        # 출력: [256 x 128 x 128]\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        # 출력: [512 x 64 x 64]\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        # 출력: [512 x 32 x 32]\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        # 출력: [512 x 16 x 16]\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        # 출력: [512 x 8 x 8]\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False,\n",
    "                              dropout=0.5)  # 출력: [512 x 4 x 4]\n",
    "\n",
    "        # 출력: [1024 x 8 x 8]\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        # 출력: [1024 x 16 x 16]\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        # 출력: [1024 x 32 x 32]\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        # 출력: [1024 x 64 x 64]\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        # 출력: [512 x 128 x 128]\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        # 출력: [256 x 256 x 256]\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        # 출력: [128 x 512 x 512]\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),  # 출력: [128 x 1024 x 1024]\n",
    "            nn.Conv2d(128, out_channels, kernel_size=3, stride=1,\n",
    "                      padding=1),  # 출력: [3 x 1024 x 1024]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)\n",
    "generator = GeneratorUNet()\n",
    "generator.to(device)\n",
    "generator.load_state_dict(torch.load(\n",
    "    '../../model/colorization/pix2pix/Pix2Pix_Generator_for_Colorization_49.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "each_device_batch =len(class_list)\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "with torch.no_grad():\n",
    "    lab = torch.ones(len(class_list), each_device_batch // len(class_list)).type(torch.long) \\\n",
    "    * torch.arange(start = 0, end = len(class_list)).reshape(-1, 1)\n",
    "    lab = lab.reshape(-1, 1).squeeze()\n",
    "    lab = lab.to(device)\n",
    "    cemb = cemblayer(lab)\n",
    "    genshape = (each_device_batch ,params['outch'], params['image_size'], params['image_size'])\n",
    "    if params['ddim']:\n",
    "        generated = diffusion.ddim_sample(\n",
    "            genshape, 10, 0.5, 'quadratic', cemb=cemb)\n",
    "    else:\n",
    "        generated = diffusion.sample(genshape, cemb = cemb)\n",
    "    generated=transback(generated)\n",
    "    for i in range(len(lab)):\n",
    "        img_pil = topilimage(generated[i].cpu())\n",
    "        img_pil.save(f'../../result/Detail/BRIL/{class_list[lab[i]]}/{epc}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated=torch.cat([generated, generated, generated], dim=1)\n",
    "generated = transback(generator(generated.to(device)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
