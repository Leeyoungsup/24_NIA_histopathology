{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import timm\n",
    "from glob import glob\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "from torchvision.transforms import ToTensor\n",
    "import time\n",
    "import json\n",
    "import torchvision\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "nltk.download('punkt')\n",
    "tf = ToTensor()\n",
    "# Device configurationresul\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "start_time = time.time()\n",
    "print(\"Start Time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(start_time)))\n",
    "params={'image_size':1024,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':8,\n",
    "        'epochs':50,\n",
    "        'data_path':'../../data/synth/010.위암 병리 이미지 및 판독문 합성 데이터/1.데이터/',\n",
    "        'test_json':'../../data/synth/010.위암 병리 이미지 및 판독문 합성 데이터/1.데이터/3.Test/2.라벨링데이터/**/*.json',\n",
    "        'vocab_path':'../../data/synth/010.위암 병리 이미지 및 판독문 합성 데이터/1.데이터/vocab.pkl',\n",
    "        'embed_size':300,\n",
    "        'hidden_size':256,\n",
    "        'num_layers':4,}\n",
    "\n",
    "print('libray import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,data_list, data_path,image_size, caption_list, class_dataset, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.data_path=data_path\n",
    "        self.caption_list= caption_list\n",
    "        self.class_dataset=class_dataset\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.image_size=image_size\n",
    "        self.data_list=data_list\n",
    "        \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        caption = self.caption_list[index]\n",
    "        vocab = self.vocab\n",
    "        images = self.trans(self.data_list[index])\n",
    "        # Convert caption (string) to word ids.\n",
    "        path=self.data_path[index]\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return images, target,path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions,path = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets,path, lengths\n",
    "\n",
    "def idx2word(vocab, indices):\n",
    "    sentence = []\n",
    "    \n",
    "    aa=indices.cpu().numpy()\n",
    "    \n",
    "    for index in aa:\n",
    "        word = vocab.idx2word[index]\n",
    "        sentence.append(word)\n",
    "    return sentence\n",
    "def word2sentence(words_list):\n",
    "    sentence=''\n",
    "    for word in words_list:\n",
    "        if word.isalnum():\n",
    "            sentence+=' '+word\n",
    "        else:\n",
    "            sentence+=word\n",
    "    return sentence\n",
    "\n",
    "print('Data Loader import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('efficientnetv2_s')\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class AttentionMILModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(AttentionMILModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Flatten the inputs\n",
    "        inputs = inputs.view(-1, channels, height, width)\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size , 2048, 1, 1)\n",
    "        \n",
    "        # Reshape features\n",
    "        features = features.view(batch_size, -1)  # Shape: (batch_size, num_tiles, 2048)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits  \n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_heads, hidden_size, num_layers, max_seq_length=100):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_size))\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_size)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions, teacher_forcing_ratio=1.0):\n",
    "        \"\"\"\n",
    "        features: (batch_size, embed_size)\n",
    "        captions: (batch_size, max_seq_length)\n",
    "        \"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        max_seq_length = captions.size(1)\n",
    "        \n",
    "        # Output 저장을 위한 텐서 초기화\n",
    "        outputs = torch.zeros(batch_size, max_seq_length, self.vocab_size).to(features.device)\n",
    "        \n",
    "        # features를 memory로 사용\n",
    "        memory = features.unsqueeze(0)  # (1, batch_size, embed_size)\n",
    "        \n",
    "        # 첫 번째 입력 토큰은 <start> 토큰\n",
    "        input_caption = captions[:, 0].unsqueeze(1)  # (batch_size, 1)\n",
    "        \n",
    "        for t in range(1, max_seq_length):\n",
    "            # 임베딩 및 포지셔널 인코딩 적용\n",
    "            input_embedded = self.embed(input_caption) + self.positional_encoding[:, :input_caption.size(1), :]\n",
    "            input_embedded = input_embedded.permute(1, 0, 2)  # (seq_len, batch_size, embed_size)\n",
    "            \n",
    "            # 타겟 마스크 생성\n",
    "            tgt_mask = self.generate_square_subsequent_mask(input_embedded.size(0)).to(features.device)\n",
    "            \n",
    "            # Transformer 디코더에 입력\n",
    "            transformer_output = self.transformer_decoder(input_embedded, memory, tgt_mask=tgt_mask)\n",
    "            transformer_output = transformer_output.permute(1, 0, 2)\n",
    "            \n",
    "            # 현재 시간 스텝의 출력 계산\n",
    "            output = self.linear(transformer_output[:, -1, :])  # (batch_size, vocab_size)\n",
    "            outputs[:, t, :] = output  # 출력 저장\n",
    "            \n",
    "            # 다음 입력 결정 (교사 강요 비율에 따라)\n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            if use_teacher_forcing:\n",
    "                # 실제 캡션의 다음 토큰 사용\n",
    "                next_input = captions[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                # 모델의 예측 사용\n",
    "                _, predicted = output.max(1)\n",
    "                next_input = predicted.unsqueeze(1)\n",
    "            \n",
    "            # 다음 입력을 input_caption에 추가\n",
    "            input_caption = torch.cat([input_caption, next_input], dim=1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"시퀀스의 순차적인 마스크 생성\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def sample(self, features, max_seq_length=None):\n",
    "        \"\"\"Greedy Search 방식으로 시퀀스를 샘플링합니다.\"\"\"\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = self.max_seq_length\n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        sampled_ids = []\n",
    "        \n",
    "        # 첫 번째 토큰은 <start> 토큰\n",
    "        input_caption = torch.ones(batch_size, 1).long().to(features.device)\n",
    "        memory = features.unsqueeze(0)  # (1, batch_size, embed_size)\n",
    "        \n",
    "        for _ in range(max_seq_length):\n",
    "            input_embedded = self.embed(input_caption) + self.positional_encoding[:, :input_caption.size(1), :]\n",
    "            input_embedded = input_embedded.permute(1, 0, 2)\n",
    "            tgt_mask = self.generate_square_subsequent_mask(input_embedded.size(0)).to(features.device)\n",
    "            transformer_output = self.transformer_decoder(input_embedded, memory, tgt_mask=tgt_mask)\n",
    "            transformer_output = transformer_output.permute(1, 0, 2)\n",
    "            output = self.linear(transformer_output[:, -1, :])  # (batch_size, vocab_size)\n",
    "            _, predicted = output.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            input_caption = torch.cat([input_caption, predicted.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        return sampled_ids\n",
    "def bleu_n(pred_words_list,label_words_list):\n",
    "\n",
    "    bleu1 = sentence_bleu([label_words_list], pred_words_list, weights=(1, 0, 0, 0))\n",
    "\n",
    "\n",
    "# BLEU@2 calculation\n",
    "    bleu2 = sentence_bleu([label_words_list], pred_words_list, weights=(0.5, 0.5, 0, 0))\n",
    "\n",
    "\n",
    "    bleu3=sentence_bleu([label_words_list], pred_words_list, weights=(0.33, 0.33, 0.33, 0))\n",
    "\n",
    "\n",
    "    bleu4=sentence_bleu([label_words_list], pred_words_list, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    return bleu1,bleu2,bleu3,bleu4\n",
    "\n",
    "def rouge_scores(pred_sentence, label_sentence):\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    scores = scorer.score(label_sentence, pred_sentence)\n",
    "    \n",
    "    # Extract the precision, recall, and f1 scores for each ROUGE metric\n",
    "    rouge1 = scores['rouge1']\n",
    "    rouge2 = scores['rouge2']\n",
    "    rougeL = scores['rougeL']\n",
    "    \n",
    "    # Return the ROUGE scores\n",
    "    return rouge1, rouge2, rougeL\n",
    "\n",
    "print('Model import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(params['vocab_path'], 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "transform = transforms.Compose([ \n",
    "        transforms.RandomCrop(params['image_size']),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_json_list=glob(params['test_json'])\n",
    "random.shuffle(test_json_list)\n",
    "test_image_list=[f.replace('2.라벨링데이터', '1.원천데이터') for f in test_json_list]\n",
    "test_image_list=[f.replace('.json', '.png') for f in test_image_list]\n",
    "test_caption_list=[]\n",
    "test_list=torch.zeros(len(test_json_list),3,params['image_size'],params['image_size'])\n",
    "for i in tqdm(range(len(test_image_list))):\n",
    "    image=transform(Image.open(test_image_list[i]).resize((params['image_size'],params['image_size'])))\n",
    "    test_list[i]=image\n",
    "    with open(test_json_list[i], 'r', encoding='utf-8-sig') as file:\n",
    "        data = json.load(file)\n",
    "    test_caption_list.append(str(data['content']['file']['patch_discription']))\n",
    "\n",
    "test_dataset=CustomDataset(test_list,test_image_list,params['image_size'],test_caption_list,'val',vocab,transform=transform)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=params['batch_size'],shuffle=False,collate_fn=collate_fn)\n",
    "print('Data Load Done')\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "encoder = AttentionMILModel(params['embed_size'], 1280, Feature_Extractor).to(device)\n",
    "decoder = DecoderTransformer(params['embed_size'], len(vocab), 15, params['hidden_size'], params['num_layers']).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_param = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_param, lr=params['lr'], betas=(params['beta1'], params['beta2']))\n",
    "encoder.load_state_dict(torch.load('../../model/captioning/ST_encoder_check.pth',map_location=device))\n",
    "decoder.load_state_dict(torch.load('../../model/captioning/ST_decoder_check.pth',map_location=device))\n",
    "print('Model Load Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    topilimage = torchvision.transforms.ToPILImage()\n",
    "    image_data = Image.open(test_image_list[-1-i])\n",
    "\n",
    "\n",
    "    # 서브플롯 설정 (1행, 2열)\n",
    "    wrapped_text = \"\\n\".join(textwrap.wrap(test_caption_list[-1-i], width=50))\n",
    "\n",
    "    # 서브플롯 설정 (1행, 2열)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # 첫 번째 서브플롯에 이미지 표시\n",
    "    ax1.imshow(image_data, cmap='gray')\n",
    "\n",
    "    ax1.set_title('Image')\n",
    "\n",
    "    # 두 번째 서브플롯에 줄바꿈된 텍스트 표시\n",
    "    ax2.text(0.5, 0.5, wrapped_text, va='center', ha='center', fontsize=12)\n",
    "    ax2.axis('off')  # 축 숨김\n",
    "    ax2.set_title('Text')\n",
    "    # 화면에 표시\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_labels = []\n",
    "total_predictions = []\n",
    "total_bleu=[]\n",
    "total_Rogue=[]\n",
    "test_df=pd.DataFrame(columns=['Path','Predicted','label','BLEU1','BLEU2','BLEU3','BLEU4','Rouge1','Rouge2','RougeL'])\n",
    "with torch.no_grad():\n",
    "    test_count = 0\n",
    "    test_loss = 0.0 \n",
    "    test_bleu_score = 0.0\n",
    "    test_tq = tqdm(test_dataloader)\n",
    "    for images, captions,path, lengths in test_tq:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Encoder를 통해 특징 추출\n",
    "        features = encoder(images)\n",
    "        # 캡션 생성 (교사 강요 없이)\n",
    "        sampled_ids = decoder.sample(features)\n",
    "        \n",
    "        # BLEU 점수 계산\n",
    "        for i in range(images.size(0)):\n",
    "            test_count += 1\n",
    "            predicted_caption = idx2word(vocab, sampled_ids[i])\n",
    "            target_caption = idx2word(vocab, captions[i])\n",
    "            \n",
    "            # 특수 토큰 제거\n",
    "            predicted_caption = [word for word in predicted_caption if word not in ['<start>', '<end>', '<pad>']]\n",
    "            target_caption = [word for word in target_caption if word not in ['<start>', '<end>', '<pad>']]\n",
    "            predicted_sentence=word2sentence(predicted_caption)\n",
    "            target_sentence=word2sentence(target_caption)\n",
    "            # BLEU-4 점수 계산\n",
    "            bleu_score = sentence_bleu([target_caption], predicted_caption, weights=(1, 0, 0, 0))\n",
    "            test_bleu_score += bleu_score\n",
    "            bleu_11=bleu_n(predicted_caption,target_caption)\n",
    "            total_bleu.append(bleu_11)\n",
    "            # ROUGE 점수 계산\n",
    "            rouge_score = rouge_scores(predicted_sentence, target_sentence)\n",
    "            total_Rogue.append(rouge_score)\n",
    "            # 예측과 정답 문장 저장\n",
    "            total_labels.append(target_sentence)\n",
    "            total_predictions.append(predicted_sentence)\n",
    "\n",
    "            test_df.loc[len(test_df)]={'Path':os.path.basename(path[i]),'Predicted':predicted_sentence,'label':target_sentence,'BLEU1':bleu_11[0],'BLEU2':bleu_11[1],'BLEU3':bleu_11[2],'BLEU4':bleu_11[3],'Rouge1':rouge_score[0],'Rouge2':rouge_score[1],'RougeL':rouge_score[2]}\n",
    "        test_tq.set_description(f\"test BLEU-1: {test_bleu_score/(test_count):.4f}\")\n",
    "test_df.to_csv('../../result/caption_result/BR_result.csv',index=False)\n",
    "end_time = time.time()\n",
    "print(\"\\nEnd Time:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(end_time)))\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "print('Test Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Bleu-1:{np.array(total_bleu)[:,0].mean():.4f}+-{np.array(total_bleu)[:,0].std():.4f} \\nBleu-2:{np.array(total_bleu)[:,1].mean():.4f}+-{np.array(total_bleu)[:,1].std():.4f} \\nBleu-3:{np.array(total_bleu)[:,2].mean():.4f}+-{np.array(total_bleu)[:,2].std():.4f} \\nBleu-4:{np.array(total_bleu)[:,3].mean():.4f}+-{np.array(total_bleu)[:,3].std():.4f} \\nRogue-1:{np.array(total_Rogue)[:,0].mean():.4f}+-{np.array(total_Rogue)[:,0].std():.4f} \\nRogue-2:{np.array(total_Rogue)[:,1].mean():.4f}+-{np.array(total_Rogue)[:,1].std():.4f} \\nRogue-L:{np.array(total_Rogue)[:,2].mean():.4f}+-{np.array(total_Rogue)[:,2].std():.4f}')\n",
    "print('performance Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    k=-1\n",
    "    image_data = image_data = Image.open(test_image_list[k-i])\n",
    "\n",
    "    # 텍스트 내용\n",
    "\n",
    "    text_content =total_labels[k-i]\n",
    "    # 서브플롯 설정 (1행, 2열)\n",
    "    wrapped_text = \"\\n\".join(textwrap.wrap(text_content, width=50))\n",
    "\n",
    "    # 서브플롯 설정 (1행, 2열)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    # 첫 번째 서브플롯에 이미지 표시\n",
    "    axs[0,0].imshow(image_data)\n",
    "\n",
    "    axs[0,0].set_title('Image')\n",
    "\n",
    "    # 두 번째 서브플롯에 줄바꿈된 텍스트 표시\n",
    "    axs[0,1].text(0.5, 0.5, wrapped_text, va='center', ha='center', fontsize=12)\n",
    "    axs[0,1].axis('off')  # 축 숨김\n",
    "    axs[0,1].set_title('GT')\n",
    "\n",
    "    wrapped_text = \"\\n\".join(textwrap.wrap(total_predictions[k-i], width=50))\n",
    "    # 첫 번째 서브플롯에 이미지 표시\n",
    "    axs[1,0].imshow(image_data)\n",
    "\n",
    "    axs[1,0].set_title('Image')\n",
    "\n",
    "    # 두 번째 서브플롯에 줄바꿈된 텍스트 표시\n",
    "    axs[1,1].text(0.5, 0.5, wrapped_text, va='center', ha='center', fontsize=12)\n",
    "    axs[1,1].axis('off')  # 축 숨김\n",
    "    axs[1,1].set_title('Predicted')\n",
    "    # 화면에 표시\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
