{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs used:\t8\n",
      "Device:\t\tcuda:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import Unet\n",
    "from conditionDiffusion.embedding import ConditionalEmbedding\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",1)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "tf=transforms.ToTensor()\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "import pytorch_model_summary as tms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list=['유형1','유형2','유형3','유형4','유형5','유형6','유형7','유형8','유형9','유형10','유형11','유형12','유형13','유형14','유형15']\n",
    "params={'image_size':1024,\n",
    "        'lr':2e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'data_path':'../../data/normalization_type/BR_mask/**/',\n",
    "        'image_count':5000,\n",
    "        'inch':6,\n",
    "        'modch': 128,\n",
    "        'outch': 6,\n",
    "        'chmul': [1, 2,4,4],\n",
    "        'numres':2,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':100,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':1,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 14.96it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.90it/s]\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self, parmas, images, label):\n",
    "\n",
    "        self.images = images\n",
    "        self.args = parmas\n",
    "        self.label = label\n",
    "\n",
    "    def trans(self, image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.label[index]\n",
    "        image = self.trans(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_label = []\n",
    "image_path = []\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list = glob(params['data_path']+class_list[i]+'/*.npy')\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "        image_label.append(i)\n",
    "train_label=torch.zeros((len(image_path),params['inch'],params['image_size'],params['image_size']))      \n",
    "for i in tqdm(range(10)):\n",
    "    npy_label = np.load(image_path[i].replace(\n",
    "        'image', 'mask').replace('jpeg', 'npy'))\n",
    "    for j in range(params['inch']):\n",
    "        train_label[i, j] = torch.tensor(npy_label == j).float()*2-1\n",
    "train_dataset=CustomDataset(params,train_label,image_label)\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Unet(in_ch = params['inch'],\n",
    "            mod_ch = params['modch'],\n",
    "            out_ch = params['outch'],\n",
    "            ch_mul = params['chmul'],\n",
    "            num_res_blocks = params['numres'],\n",
    "            cdim = params['cdim'],\n",
    "            use_conv = params['useconv'],\n",
    "            droprate = params['droprate'],\n",
    "            dtype = params['dtype']\n",
    "            ).to(device)\n",
    "betas = get_named_beta_schedule(num_diffusion_timesteps = params['T'])\n",
    "cemblayer = ConditionalEmbedding(\n",
    "    len(class_list), params['cdim'], params['cdim']).to(device)\n",
    "diffusion = GaussianDiffusion(\n",
    "                    dtype = params['dtype'],\n",
    "                    model = net,\n",
    "                    betas = betas,\n",
    "                    w = params['w'],\n",
    "                    v = params['v'],\n",
    "                    device = device\n",
    "                )\n",
    "optimizer = torch.optim.AdamW(\n",
    "                itertools.chain(\n",
    "                    diffusion.model.parameters(),\n",
    "                    cemblayer.parameters()\n",
    "                ),\n",
    "                lr = params['lr'],\n",
    "                weight_decay = 1e-6\n",
    "            )\n",
    "\n",
    "\n",
    "cosineScheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "warmUpScheduler = GradualWarmupScheduler(\n",
    "                        optimizer = optimizer,\n",
    "                        multiplier = params['multiplier'],\n",
    "                        warm_epoch = 50,\n",
    "                        after_scheduler = cosineScheduler,\n",
    "                        last_epoch = 0\n",
    "                    )\n",
    "# checkpoint=torch.load(f'../../model/conditionDiff/BR/ckpt_35_checkpoint.pt',map_location=device)\n",
    "# diffusion.model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "checkpoint=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9179 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU \u0001 has a total capacity of 79.25 GiB of which 634.62 MiB is free. Process 303021 has 14.32 GiB memory in use. Including non-PyTorch memory, this process has 64.28 GiB memory in use. Of the allocated memory 63.29 GiB is allocated by PyTorch, and 505.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m lab \u001b[38;5;241m=\u001b[39m lab\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m cemb \u001b[38;5;241m=\u001b[39m cemblayer(lab)\n\u001b[0;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/gcubme_ai2/Workspace/YS_Lee/2024_NIA_histopathology/code/24_NIA_histopathology/conditionDiffusion/diffusion.py:254\u001b[0m, in \u001b[0;36mGaussianDiffusion.trainloss\u001b[0;34m(self, x_0, **model_kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT, size \u001b[38;5;241m=\u001b[39m (x_0\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    253\u001b[0m x_t, eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(x_0, t)\n\u001b[0;32m--> 254\u001b[0m pred_eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_eps, eps, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gcubme_ai2/Workspace/YS_Lee/2024_NIA_histopathology/code/24_NIA_histopathology/conditionDiffusion/unet.py:288\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, t, cemb)\u001b[0m\n\u001b[1;32m    286\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddleblocks(h, temb, cemb)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupblocks:\n\u001b[0;32m--> 288\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     h \u001b[38;5;241m=\u001b[39m block(h, temb, cemb)\n\u001b[1;32m    290\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU \u0001 has a total capacity of 79.25 GiB of which 634.62 MiB is free. Process 303021 has 14.32 GiB memory in use. Including non-PyTorch memory, this process has 64.28 GiB memory in use. Of the allocated memory 63.29 GiB is allocated by PyTorch, and 505.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epc in range(params['epochs']):\n",
    "    diffusion.model.train()\n",
    "    total_loss=0\n",
    "    steps=0\n",
    "    \n",
    "    with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, lab in tqdmDataLoader:\n",
    "            b = img.shape[0]\n",
    "\n",
    "            x_0 = img.to(device)\n",
    "            lab = lab.to(device)\n",
    "            cemb = cemblayer(lab)\n",
    "            loss = diffusion.trainloss(x_0, cemb=cemb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            steps += 1\n",
    "            total_loss += loss.item()\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"loss: \": total_loss/steps,\n",
    "                    \"batch per device: \": x_0.shape[0],\n",
    "                    \"img shape: \": x_0.shape[1:],\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                }\n",
    "            )\n",
    "    warmUpScheduler.step()\n",
    "\n",
    "    diffusion.model.eval()\n",
    "    cemblayer.eval()\n",
    "    # generating samples\n",
    "    # The model generate 80 pictures(8 per row) each time\n",
    "    # pictures of same row belong to the same class\n",
    "    all_samples = []\n",
    "    each_device_batch =len(class_list)\n",
    "    count=1\n",
    "    with torch.no_grad():\n",
    "        for i, (img, lab1) in enumerate(dataloader):\n",
    "            if i==0:\n",
    "                lab = lab1.to(device)\n",
    "            elif i < count:\n",
    "                lab = torch.cat((lab,lab1.to(device)),0).to(device)\n",
    "            else:\n",
    "                break\n",
    "        cemb =cemblayer(lab)\n",
    "        genshape = (count, 6, params['image_size'], params['image_size'])\n",
    "        if params['ddim']:\n",
    "            generated = diffusion.ddim_sample(\n",
    "                genshape, 20, 0.5, 'quadratic', cemb=cemb)\n",
    "        else:\n",
    "            generated = diffusion.sample(genshape, cemb = cemb)\n",
    "        generated=transback(generated)\n",
    "        generated=generated.cpu()\n",
    "        for i in range(len(lab)):\n",
    "            img_tensor=torch.zeros((3,params['image_size'],params['image_size']))\n",
    "            img_tensor[0]=torch.where(generated[i,1]>0,1,0)\n",
    "            img_tensor[1]=torch.where(generated[i,2]>0,1,0)\n",
    "            img_tensor[2]=torch.where(generated[i,3]>0,1,0)\n",
    "            img_tensor[0]=torch.where(generated[i,4]>0,1,0)\n",
    "            img_tensor[1]=torch.where(generated[i,4]>0,1,0)\n",
    "            img_tensor[1]=torch.where(generated[i,5]>0,1,0)\n",
    "            img_tensor[2]=torch.where(generated[i,5]>0,1,0)\n",
    "            \n",
    "            img_pil = topilimage(img_tensor)\n",
    "            img_pil.save(f'../../result/mask_synth/BR_mask/{epc}_{i}.png')\n",
    "\n",
    "        # save checkpoints\n",
    "        checkpoint = {\n",
    "            'net': diffusion.model.state_dict(),\n",
    "            'cemblayer': cemblayer.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': warmUpScheduler.state_dict()\n",
    "        }\n",
    "    torch.save(checkpoint, f'../../model/mask_synth/BR_mask/ckpt_{epc+1}_checkpoint.pt')\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start generating(ddim)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:26<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending sampling process(ddim)...\n"
     ]
    }
   ],
   "source": [
    "count=1\n",
    "with torch.no_grad():\n",
    "    for i, (img, lab1) in enumerate(dataloader):\n",
    "        if i==0:\n",
    "            lab = lab1.to(device)\n",
    "        elif i < count:\n",
    "            lab = torch.cat((lab,lab1.to(device)),0).to(device)\n",
    "        else:\n",
    "            break\n",
    "    cemb =cemblayer(lab)\n",
    "    genshape = (count, 6, params['image_size'], params['image_size'])\n",
    "    if params['ddim']:\n",
    "        generated = diffusion.ddim_sample(\n",
    "            genshape, 20, 0.5, 'quadratic', cemb=cemb)\n",
    "    else:\n",
    "        generated = diffusion.sample(genshape, cemb = cemb)\n",
    "    generated=transback(generated)\n",
    "    generated=generated.cpu()\n",
    "    for i in range(len(lab)):\n",
    "        img_tensor=torch.zeros((3,params['image_size'],params['image_size']))\n",
    "        img_tensor[0]=torch.where(generated[i,1]>0,1,0)\n",
    "        img_tensor[1]=torch.where(generated[i,2]>0,1,0)\n",
    "        img_tensor[2]=torch.where(generated[i,3]>0,1,0)\n",
    "        img_tensor[0]=torch.where(generated[i,4]>0,1,0)\n",
    "        img_tensor[1]=torch.where(generated[i,4]>0,1,0)\n",
    "        img_tensor[1]=torch.where(generated[i,5]>0,1,0)\n",
    "        img_tensor[2]=torch.where(generated[i,5]>0,1,0)\n",
    "        \n",
    "        img_pil = topilimage(img_tensor)\n",
    "        img_pil.save(f'../../result/mask_synth/BR_mask/{epc}_{i}.png')\n",
    "\n",
    "    # save checkpoints\n",
    "    checkpoint = {\n",
    "        'net': diffusion.model.state_dict(),\n",
    "        'cemblayer': cemblayer.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': warmUpScheduler.state_dict()\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
