{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import Unet\n",
    "from conditionDiffusion.embedding import ConditionalEmbedding\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",6)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "import pytorch_model_summary as tms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list=['유형1','유형2']\n",
    "params={'image_size':1024,\n",
    "        'lr':1e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'data_path':'../../data/origin_type/STNT/',\n",
    "        'image_count':5000,\n",
    "        'inch':3,\n",
    "        'modch':32,\n",
    "        'outch':3,\n",
    "        'chmul':[1,2,4,8,16,32,32],\n",
    "        'numres':2,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':10,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':2.5,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        image = self.trans(image)\n",
    "        return image,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list=glob(params['data_path']+class_list[i]+'/*.jpeg')\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "        image_label.append(i)\n",
    "        \n",
    "train_images=torch.zeros((len(image_path),params['inch'],params['image_size'],params['image_size']))\n",
    "for i in tqdm(range(len(image_path))):\n",
    "    train_images[i]=trans(Image.open(image_path[i]).convert('RGB').resize((params['image_size'],params['image_size'])))\n",
    "train_dataset=CustomDataset(params,train_images,image_label)\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Unet(in_ch = params['inch'],\n",
    "            mod_ch = params['modch'],\n",
    "            out_ch = params['outch'],\n",
    "            ch_mul = params['chmul'],\n",
    "            num_res_blocks = params['numres'],\n",
    "            cdim = params['cdim'],\n",
    "            use_conv = params['useconv'],\n",
    "            droprate = params['droprate'],\n",
    "            dtype = params['dtype']\n",
    "            ).to(device)\n",
    "cemblayer = ConditionalEmbedding(len(class_list), params['cdim'], params['cdim']).to(device)\n",
    "betas = get_named_beta_schedule(num_diffusion_timesteps = params['T'])\n",
    "diffusion = GaussianDiffusion(\n",
    "                    dtype = params['dtype'],\n",
    "                    model = net,\n",
    "                    betas = betas,\n",
    "                    w = params['w'],\n",
    "                    v = params['v'],\n",
    "                    device = device\n",
    "                )\n",
    "optimizer = torch.optim.AdamW(\n",
    "                itertools.chain(\n",
    "                    diffusion.model.parameters(),\n",
    "                    cemblayer.parameters()\n",
    "                ),\n",
    "                lr = params['lr'],\n",
    "                weight_decay = 1e-4\n",
    "            )\n",
    "\n",
    "\n",
    "cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                            optimizer = optimizer,\n",
    "                            T_max = params['epochs']/100,\n",
    "                            eta_min = 0,\n",
    "                            last_epoch = -1\n",
    "                        )\n",
    "warmUpScheduler = GradualWarmupScheduler(\n",
    "                        optimizer = optimizer,\n",
    "                        multiplier = params['multiplier'],\n",
    "                        warm_epoch = params['epochs'] // 10,\n",
    "                        after_scheduler = cosineScheduler,\n",
    "                        last_epoch = 0\n",
    "                    )\n",
    "\n",
    "checkpoint=torch.load(f'../../model/conditionDiff/details/STNT/ckpt_231_checkpoint.pt',map_location=device)\n",
    "diffusion.model.load_state_dict(checkpoint['net'])\n",
    "cemblayer.load_state_dict(checkpoint['cemblayer'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "warmUpScheduler.load_state_dict(checkpoint['scheduler'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1462/1462 [13:01<00:00,  1.87it/s, epoch=505, loss: =0.0247, batch per device: =1, img shape: =torch.Size([3, 1024, 1024]), LR=1.64e-5]\n",
      "100%|██████████| 1462/1462 [12:57<00:00,  1.88it/s, epoch=506, loss: =0.0255, batch per device: =1, img shape: =torch.Size([3, 1024, 1024]), LR=1.25e-5]\n",
      "100%|██████████| 1462/1462 [13:00<00:00,  1.87it/s, epoch=507, loss: =0.0242, batch per device: =1, img shape: =torch.Size([3, 1024, 1024]), LR=8.64e-6]\n",
      " 82%|████████▏ | 1201/1462 [10:46<02:20,  1.86it/s, epoch=508, loss: =0.0228, batch per device: =1, img shape: =torch.Size([3, 1024, 1024]), LR=5.15e-6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m cemb[np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(b)\u001b[38;5;241m<\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m])] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m diffusion\u001b[38;5;241m.\u001b[39mtrainloss(x_0, cemb \u001b[38;5;241m=\u001b[39m cemb)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epc in range(230,params['epochs']):\n",
    "    diffusion.model.train()\n",
    "    cemblayer.train()\n",
    "    total_loss=0\n",
    "    steps=0\n",
    "    with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, lab in tqdmDataLoader:\n",
    "            b = img.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            x_0 = img.to(device)\n",
    "            lab = lab.to(device)\n",
    "            cemb = cemblayer(lab)\n",
    "            cemb[np.where(np.random.rand(b)<params['threshold'])] = 0\n",
    "            loss = diffusion.trainloss(x_0, cemb = cemb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps+=1\n",
    "            total_loss+=loss.item()\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"loss: \": total_loss/steps,\n",
    "                    \"batch per device: \":x_0.shape[0],\n",
    "                    \"img shape: \": x_0.shape[1:],\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                }\n",
    "            )\n",
    "    warmUpScheduler.step()\n",
    "    if (epc) % 10 == 0:\n",
    "        diffusion.model.eval()\n",
    "        cemblayer.eval()\n",
    "        all_samples = []\n",
    "        each_device_batch =10\n",
    "        with torch.no_grad():\n",
    "            lab = torch.ones(len(class_list), each_device_batch // len(class_list)).type(torch.long) \\\n",
    "            * torch.arange(start = 0, end = len(class_list)).reshape(-1, 1)\n",
    "            lab = lab.reshape(-1, 1).squeeze()\n",
    "            lab = lab.to(device)\n",
    "            cemb = cemblayer(lab)\n",
    "            genshape = (each_device_batch , 3, params['image_size'], params['image_size'])\n",
    "            if params['ddim']:\n",
    "                generated = diffusion.ddim_sample(genshape, 50, 0, 'linear', cemb = cemb)\n",
    "            else:\n",
    "                generated = diffusion.sample(genshape, cemb = cemb)\n",
    "            img = transback(generated)\n",
    "            img = img.reshape(len(class_list), each_device_batch // len(class_list), 3, params['image_size'], params['image_size']).contiguous()\n",
    "            all_samples.append(img)\n",
    "            samples = torch.concat(all_samples, dim = 1).reshape(each_device_batch, 3, params['image_size'], params['image_size'])\n",
    "\n",
    "        save_image(samples,f'../../result/Detail/STNT/generated_{epc+1}_pict.png', nrow = each_device_batch // len(class_list))\n",
    "        # save checkpoints\n",
    "        checkpoint = {\n",
    "                            'net':diffusion.model.state_dict(),\n",
    "                            'cemblayer':cemblayer.state_dict(),\n",
    "                            'optimizer':optimizer.state_dict(),\n",
    "                            'scheduler':warmUpScheduler.state_dict()\n",
    "                        }\n",
    "        torch.save(checkpoint, f'../../model/conditionDiff/details/STNT/ckpt_{epc+1}_checkpoint.pt')\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
