{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/onnx/_internal/_beartype.py:36: UserWarning: unhashable type: 'list'\n",
      "  warnings.warn(f\"{e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs used:\t8\n",
      "Device:\t\tcuda:4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import Unet\n",
    "from conditionDiffusion.embedding import ConditionalEmbedding\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import styleGAN.networks_stylegan as stylegan\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",4)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "import pytorch_model_summary as tms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list=['유형1','유형2']\n",
    "params={'image_size':1024,\n",
    "        'lr':1e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'data_path':'../../data/origin_type/STNT/',\n",
    "        'image_count':5000,\n",
    "        'inch':3,\n",
    "        'modch':64,\n",
    "        'outch':3,\n",
    "        'chmul':[1,2,4,8,16,16,16],\n",
    "        'numres':2,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':10,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':2.5,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 24.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m         image_path\u001b[38;5;241m.\u001b[39mappend(image_list[j])\n\u001b[1;32m     44\u001b[0m         image_label\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m---> 46\u001b[0m train_images\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(image_path))):\n\u001b[1;32m     48\u001b[0m     train_images[i]\u001b[38;5;241m=\u001b[39mtrans(Image\u001b[38;5;241m.\u001b[39mopen(image_path[i])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m],params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        image = self.trans(image)\n",
    "        return image,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list=glob(params['data_path']+class_list[i]+'/*.jpeg')\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "        image_label.append(i)\n",
    "        \n",
    "train_images=torch.zeros((len(image_path),params['inch'],params['image_size'],params['image_size']))\n",
    "for i in tqdm(range(len(image_path))):\n",
    "    train_images[i]=trans(Image.open(image_path[i]).convert('RGB').resize((params['image_size'],params['image_size'])))\n",
    "train_dataset=CustomDataset(params,train_images,image_label)\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer Output Size: torch.Size([1, 512, 36, 36])\n",
      "Layer L0_36_512 Output Size: torch.Size([1, 512, 55, 55])\n",
      "Layer L1_36_512 Output Size: torch.Size([1, 512, 74, 74])\n",
      "Layer L2_52_512 Output Size: torch.Size([1, 512, 61, 61])\n",
      "Layer L3_52_512 Output Size: torch.Size([1, 512, 80, 80])\n",
      "Layer L4_84_512 Output Size: torch.Size([1, 512, 67, 67])\n",
      "Layer L5_84_512 Output Size: torch.Size([1, 512, 86, 86])\n",
      "Layer L6_148_512 Output Size: torch.Size([1, 512, 73, 73])\n",
      "Layer L7_148_512 Output Size: torch.Size([1, 512, 92, 92])\n",
      "Layer L8_276_512 Output Size: torch.Size([1, 512, 79, 79])\n",
      "Layer L9_276_431 Output Size: torch.Size([1, 431, 98, 98])\n",
      "Layer L10_532_287 Output Size: torch.Size([1, 287, 85, 85])\n",
      "Layer L11_532_192 Output Size: torch.Size([1, 192, 104, 104])\n",
      "Layer L12_532_128 Output Size: torch.Size([1, 128, 123, 123])\n",
      "Layer L13_512_128 Output Size: torch.Size([1, 128, 102, 102])\n",
      "Layer L14_512_3 Output Size: torch.Size([1, 3, 102, 102])\n",
      "Final Output Size: torch.Size([1, 3, 102, 102])\n",
      "---------------------------------------------------------------------------\n",
      "         Layer (type)         Output Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "          Embedding-1              [1, 10]             100             100\n",
      "     MappingNetwork-2         [1, 16, 512]       2,106,368       2,106,368\n",
      "   SynthesisNetwork-3     [1, 3, 102, 102]      28,692,636      28,692,636\n",
      "===========================================================================\n",
      "Total params: 30,799,104\n",
      "Trainable params: 30,799,104\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generator = stylegan.Generator(\n",
    "    z_dim=512,              # 잠재 벡터 Z의 차원\n",
    "    c_dim=10,               # 조건 레이블 C의 차원 (10개의 클래스)\n",
    "    w_dim=512,              # 중간 잠재 벡터 W의 차원\n",
    "    img_resolution=512,     # 출력 이미지 해상도 (512x512)\n",
    "    img_channels=3,         # 출력 이미지의 채널 수 (RGB 이미지)\n",
    "    num_classes=10,         # 데이터셋의 클래스 개수\n",
    "    mapping_kwargs={\"lr_multiplier\": 0.01},  # MappingNetwork에 전달할 추가 인자 \n",
    ")\n",
    "z = torch.randn(1, 512)  # 잠재 벡터\n",
    "y = torch.randint(0, 10, (1,))  # 임의의 레이블\n",
    "\n",
    "# 모델 구조를 출력합니다.\n",
    "print(tms.summary(generator, torch.zeros(1, 512), torch.zeros(1, dtype=torch.int64), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m img_resolution \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     44\u001b[0m img_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 46\u001b[0m synthesis_network \u001b[38;5;241m=\u001b[39m \u001b[43mSynthesisNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m512\u001b[39m)  \u001b[38;5;66;03m# Example latent vector\u001b[39;00m\n\u001b[1;32m     48\u001b[0m output_img \u001b[38;5;241m=\u001b[39m synthesis_network(w)\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mSynthesisNetwork.__init__\u001b[0;34m(self, w_dim, img_resolution, img_channels)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_rgb_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# 수정된 부분: 범위를 3부터 시작\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     in_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m     out_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels[\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (res \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mConv2d(in_channels, out_channels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SynthesisNetwork(nn.Module):\n",
    "    def __init__(self, w_dim, img_resolution, img_channels):\n",
    "        super().__init__()\n",
    "        self.w_dim = w_dim\n",
    "        self.img_resolution = img_resolution\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        # Define the layers based on the resolution\n",
    "        self.num_layers = int(np.log2(img_resolution)) * 2 - 2\n",
    "        self.channels = {4: 512, 8: 512, 16: 512, 32: 512, 64: 256, 128: 128, 256: 64, 512: 32, 1024: 16}\n",
    "\n",
    "        self.input_layer = nn.Parameter(torch.randn(1, self.channels[4], 4, 4))\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.to_rgb_layers = nn.ModuleList()\n",
    "\n",
    "        for res in range(3, self.num_layers + 3):  # 수정된 부분: 범위를 3부터 시작\n",
    "            in_channels = self.channels[2 ** (res - 2)]\n",
    "            out_channels = self.channels[2 ** (res - 1)]\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            self.conv_layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "            self.to_rgb_layers.append(nn.Conv2d(out_channels, img_channels, kernel_size=1))\n",
    "\n",
    "    def forward(self, w):\n",
    "        x = self.input_layer.repeat(w.shape[0], 1, 1, 1)\n",
    "        for i in range(0, len(self.conv_layers), 2):\n",
    "            x = F.leaky_relu(self.conv_layers[i](x), 0.2)\n",
    "            x = F.leaky_relu(self.conv_layers[i + 1](x), 0.2)\n",
    "            if i // 2 < len(self.to_rgb_layers):\n",
    "                rgb = self.to_rgb_layers[i // 2](x)\n",
    "                if i == 0:\n",
    "                    img = rgb\n",
    "                else:\n",
    "                    img = F.interpolate(img, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "                    img = img + rgb\n",
    "        return img\n",
    "\n",
    "# Example usage\n",
    "w_dim = 512\n",
    "img_resolution = 512\n",
    "img_channels = 3\n",
    "\n",
    "synthesis_network = SynthesisNetwork(w_dim, img_resolution, img_channels)\n",
    "w = torch.randn(1, 16, 512)  # Example latent vector\n",
    "output_img = synthesis_network(w)\n",
    "print(output_img.shape)  # Should be [1, 3, 512, 512]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
