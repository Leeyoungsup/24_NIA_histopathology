{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import time\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import os\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm import create_model\n",
    "import cv2\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=2\n",
    "img_size=1024\n",
    "class_list=['NT_epithelial','NT_immune','NT_stroma','TP_in_situ','TP_invasive']\n",
    "class_nm='TP_invasive'\n",
    "tf = ToTensor()\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='../../data/area_segmentation/BR_class/image/'\n",
    "img_list=glob(img_path+'NIA6_R_BRID*.jpeg')\n",
    "mask_list=[i.replace('/image','/'+class_nm) for i in img_list]\n",
    "train_img_list,test_img_list,train_mask_list,test_mask_list=train_test_split(img_list,mask_list,test_size=0.2,random_state=42)\n",
    "\n",
    "test_image=torch.zeros((len(test_img_list),3,img_size,img_size))\n",
    "test_mask=torch.zeros((len(test_img_list),2,img_size,img_size),dtype=torch.float32)    \n",
    "train_image=torch.zeros((len(train_img_list),3,img_size,img_size))\n",
    "train_mask=torch.zeros((len(train_img_list),2,img_size,img_size),dtype=torch.float32)\n",
    "for i in tqdm(range(len(train_img_list))):\n",
    "    train_image[i] = tf(Image.open(train_img_list[i]))\n",
    "    np_mask=tf(Image.open(train_mask_list[i]))\n",
    "    train_mask[i,1]=np_mask\n",
    "    train_mask[i,0]=1-np_mask\n",
    "for i in tqdm(range(len(test_img_list))):\n",
    "    test_image[i] = tf(Image.open(test_img_list[i]))\n",
    "    np_mask=tf(Image.open(test_mask_list[i]))\n",
    "    test_mask[i,1]=np_mask\n",
    "    test_mask[i,0]=1-np_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         image_path,label_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path[idx],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel[idx])\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image_path, label_path\n\u001b[0;32m---> 27\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mtrain_image\u001b[49m, train_mask)\n\u001b[1;32m     29\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_image, test_mask)\n\u001b[1;32m     30\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_image' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "        \n",
    "    def trans(self,image,label):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            label = transform(label)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            label = transform(label)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path,label_path = self.trans(self.img_path[idx],self.label[idx])\n",
    "\n",
    "        return image_path, label_path\n",
    "    \n",
    "train_dataset = CustomDataset(train_image, train_mask)\n",
    "\n",
    "test_dataset = CustomDataset(test_image, test_mask)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "UnetPlusPlus                                            [2, 2, 1024, 1024]        --\n",
       "├─EfficientNetEncoder: 1-1                              [2, 3, 1024, 1024]        1,643,520\n",
       "│    └─Conv2dStaticSamePadding: 2-1                     [2, 64, 512, 512]         1,728\n",
       "│    │    └─ZeroPad2d: 3-1                              [2, 3, 1025, 1025]        --\n",
       "│    └─BatchNorm2d: 2-2                                 [2, 64, 512, 512]         128\n",
       "│    └─MemoryEfficientSwish: 2-3                        [2, 64, 512, 512]         --\n",
       "│    └─ModuleList: 2-4                                  --                        --\n",
       "│    │    └─MBConvBlock: 3-2                            [2, 32, 512, 512]         4,944\n",
       "│    │    └─MBConvBlock: 3-3                            [2, 32, 512, 512]         1,992\n",
       "│    │    └─MBConvBlock: 3-4                            [2, 32, 512, 512]         1,992\n",
       "│    │    └─MBConvBlock: 3-5                            [2, 32, 512, 512]         1,992\n",
       "│    │    └─MBConvBlock: 3-6                            [2, 48, 256, 256]         21,224\n",
       "│    │    └─MBConvBlock: 3-7                            [2, 48, 256, 256]         38,700\n",
       "│    │    └─MBConvBlock: 3-8                            [2, 48, 256, 256]         38,700\n",
       "│    │    └─MBConvBlock: 3-9                            [2, 48, 256, 256]         38,700\n",
       "│    │    └─MBConvBlock: 3-10                           [2, 48, 256, 256]         38,700\n",
       "│    │    └─MBConvBlock: 3-11                           [2, 48, 256, 256]         38,700\n",
       "│    │    └─MBConvBlock: 3-12                           [2, 48, 256, 256]         38,700\n",
       "│    │    └─MBConvBlock: 3-13                           [2, 80, 128, 128]         52,588\n",
       "│    │    └─MBConvBlock: 3-14                           [2, 80, 128, 128]         110,580\n",
       "│    │    └─MBConvBlock: 3-15                           [2, 80, 128, 128]         110,580\n",
       "│    │    └─MBConvBlock: 3-16                           [2, 80, 128, 128]         110,580\n",
       "│    │    └─MBConvBlock: 3-17                           [2, 80, 128, 128]         110,580\n",
       "│    │    └─MBConvBlock: 3-18                           [2, 80, 128, 128]         110,580\n",
       "│    │    └─MBConvBlock: 3-19                           [2, 80, 128, 128]         110,580\n",
       "│    │    └─MBConvBlock: 3-20                           [2, 160, 64, 64]          141,460\n",
       "│    │    └─MBConvBlock: 3-21                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-22                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-23                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-24                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-25                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-26                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-27                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-28                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-29                           [2, 160, 64, 64]          397,800\n",
       "│    │    └─MBConvBlock: 3-30                           [2, 224, 64, 64]          474,728\n",
       "│    │    └─MBConvBlock: 3-31                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-32                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-33                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-34                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-35                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-36                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-37                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-38                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-39                           [2, 224, 64, 64]          793,464\n",
       "│    │    └─MBConvBlock: 3-40                           [2, 384, 32, 32]          1,008,824\n",
       "│    │    └─MBConvBlock: 3-41                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-42                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-43                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-44                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-45                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-46                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-47                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-48                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-49                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-50                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-51                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-52                           [2, 384, 32, 32]          2,281,824\n",
       "│    │    └─MBConvBlock: 3-53                           [2, 640, 32, 32]          2,835,296\n",
       "│    │    └─MBConvBlock: 3-54                           [2, 640, 32, 32]          6,199,200\n",
       "│    │    └─MBConvBlock: 3-55                           [2, 640, 32, 32]          6,199,200\n",
       "│    │    └─MBConvBlock: 3-56                           [2, 640, 32, 32]          6,199,200\n",
       "├─UnetPlusPlusDecoder: 1-2                              [2, 16, 1024, 1024]       --\n",
       "│    └─ModuleDict: 2-5                                  --                        --\n",
       "│    │    └─DecoderBlock: 3-57                          [2, 256, 64, 64]          2,581,504\n",
       "│    │    └─DecoderBlock: 3-58                          [2, 80, 128, 128]         276,800\n",
       "│    │    └─DecoderBlock: 3-59                          [2, 48, 256, 256]         76,224\n",
       "│    │    └─DecoderBlock: 3-60                          [2, 64, 512, 512]         101,632\n",
       "│    │    └─DecoderBlock: 3-61                          [2, 128, 128, 128]        627,200\n",
       "│    │    └─DecoderBlock: 3-62                          [2, 48, 256, 256]         96,960\n",
       "│    │    └─DecoderBlock: 3-63                          [2, 64, 512, 512]         138,496\n",
       "│    │    └─DecoderBlock: 3-64                          [2, 64, 256, 256]         193,792\n",
       "│    │    └─DecoderBlock: 3-65                          [2, 64, 512, 512]         175,360\n",
       "│    │    └─DecoderBlock: 3-66                          [2, 32, 512, 512]         101,504\n",
       "│    │    └─DecoderBlock: 3-67                          [2, 16, 1024, 1024]       6,976\n",
       "├─SegmentationHead: 1-3                                 [2, 2, 1024, 1024]        --\n",
       "│    └─Conv2d: 2-6                                      [2, 2, 1024, 1024]        290\n",
       "│    └─Identity: 2-7                                    [2, 2, 1024, 1024]        --\n",
       "│    └─Activation: 2-8                                  [2, 2, 1024, 1024]        --\n",
       "│    │    └─Identity: 3-68                              [2, 2, 1024, 1024]        --\n",
       "=========================================================================================================\n",
       "Total params: 68,163,698\n",
       "Trainable params: 68,163,698\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 384.44\n",
       "=========================================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 19160.63\n",
       "Params size (MB): 18.73\n",
       "Estimated Total Size (MB): 19204.52\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2,                      # model output channels (number of classes in your dataset) \n",
    "    ).to(device)\n",
    "def dice_loss(pred, target, num_classes=2):\n",
    "    smooth = 1e-6\n",
    "    dice_per_class = torch.zeros((len(pred),num_classes)).to(pred.device)\n",
    "    pred=F.softmax(pred,dim=1)\n",
    "    for i in range(len(pred)):\n",
    "        for class_id in range(num_classes):\n",
    "            pred_class = pred[i, class_id, ...]\n",
    "            target_class = target[i, class_id, ...]\n",
    "            \n",
    "            intersection = torch.sum(pred_class * target_class)\n",
    "            A_sum = torch.sum(pred_class * pred_class)\n",
    "            B_sum = torch.sum(target_class * target_class)\n",
    "            dice_per_class[i,class_id] =(2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return 1-dice_per_class.mean()\n",
    "model.load_state_dict(torch.load('../../model/areaSeg/BR_TP_in_situ.pt',map_location=device))\n",
    "summary(model,(batch_size,3,img_size,img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "val_acc_list=[]\n",
    "model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_nm+'.pt'))\n",
    "\n",
    "MIN_loss=5000\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "metrics = defaultdict(float)\n",
    "for epoch in range(1000):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    for x, y in train:\n",
    "        model.train()\n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "        predict = model(x).to(device)\n",
    "        cost = dice_loss(predict, y) # cost 구함\n",
    "        acc=1-cost.item()\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.step() \n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        y = y.to('cpu')\n",
    "\n",
    "        x=x.to('cpu')\n",
    "        train.set_description(f\"epoch: {epoch+1}/{1000} Step: {count+1} dice_loss : {running_loss/count:.4f} dice_score: {1-running_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count))\n",
    "#test\n",
    "    val=tqdm(test_dataloader)\n",
    "    model.eval()\n",
    "    count=0\n",
    "    val_running_loss=0.0\n",
    "    acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val:\n",
    "            y = y.to(device).float()\n",
    "            count+=1\n",
    "            x=x.to(device).float()\n",
    "            \n",
    "            predict = model(x).to(device)\n",
    "            cost = dice_loss(predict, y) # cost 구함\n",
    "            acc=1-cost.item()\n",
    "            val_running_loss+=cost.item()\n",
    "            acc_loss+=acc\n",
    "            y = y.to('cpu')\n",
    "            x=x.to('cpu')\n",
    "            val.set_description(f\"test epoch: {epoch+1}/{1000} Step: {count+1} dice_loss : {val_running_loss/count:.4f}  dice_score: {1-val_running_loss/count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/count))\n",
    "        val_acc_list.append((acc_loss/count))\n",
    "        \n",
    "    if MIN_loss>(val_running_loss/count):\n",
    "        torch.save(model.state_dict(), '../../model/areaSeg/BR_'+class_nm+'.pt')\n",
    "        MIN_loss=(val_running_loss/count)\n",
    "        \n",
    "    if epoch%50==5:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1, 2, 1) \n",
    "        plt.title('loss_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "        plt.plot(np.arange(epoch+1),val_loss_list,label='test_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)  \n",
    "        plt.title('acc_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_acc_list,label='train_acc')\n",
    "        plt.plot(np.arange(epoch+1),val_acc_list,label='test_acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.title('loss_graph')\n",
    "plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "plt.plot(np.arange(epoch+1),val_loss_list,label='test_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0, 1]) \n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)  \n",
    "plt.title('acc_graph')\n",
    "plt.plot(np.arange(epoch+1),train_acc_list,label='train_acc')\n",
    "plt.plot(np.arange(epoch+1),val_acc_list,label='test_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim([0, 1]) \n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('batch size= 2')\n",
    "print('image size= 1024,1024')\n",
    "print('learning rate= 2e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../model/areaSeg/BR_'+class_nm+'_check.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
