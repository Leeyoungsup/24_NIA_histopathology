{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs used:\t8\n",
      "Device:\t\tcuda:6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from RealESRGAN import RealESRGAN\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",6)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "import pytorch_model_summary as tms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list=['BRNT','BRLC','BRIL','BRID','BRDC']\n",
    "params={'image_size':1024,\n",
    "        'lr':1e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'data_path':'../../data/NIA/',\n",
    "        'image_count':10000,\n",
    "        'inch':3,\n",
    "        'modch':64,\n",
    "        'outch':3,\n",
    "        'chmul':[1,2,4,8,16,32],\n",
    "        'numres':2,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':10,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':2.5,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        }\n",
    "\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "tf=transforms.ToTensor()\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:24<00:00,  4.98s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 예시 데이터셋 클래스 (HR, LR 이미지 쌍)\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self,params, hr_imgs,count_list, scale=2):\n",
    "        self.scale = scale\n",
    "        self.hr_images = hr_imgs\n",
    "        self.count_list=count_list\n",
    "    def __len__(self):\n",
    "        return 50000\n",
    "    \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    def random_resample(self,image, scale_factor=0.5):\n",
    "        # 사용할 보간 방법 리스트\n",
    "        image=topilimage(image)\n",
    "        resampling_methods = [\n",
    "            Image.NEAREST,\n",
    "            Image.BOX,\n",
    "            Image.BILINEAR,\n",
    "            Image.HAMMING,\n",
    "            Image.BICUBIC,\n",
    "            Image.LANCZOS\n",
    "        ]\n",
    "\n",
    "        # 랜덤하게 보간 방법 선택\n",
    "        chosen_method = random.choice(resampling_methods)\n",
    "\n",
    "        # 새로운 크기 계산\n",
    "        new_size = (\n",
    "            int(image.width * scale_factor),\n",
    "            int(image.height * scale_factor)\n",
    "        )\n",
    "\n",
    "        # 이미지를 새로운 크기로 변경\n",
    "        image = image.resize(new_size, resample=chosen_method)\n",
    "        image=tf(image)*2-1\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index//10000==0:\n",
    "            start=0\n",
    "            ind=random.randint(start,self.count_list[index//10000]-1)\n",
    "            hr_image=self.trans(tf(Image.open(self.hr_images[ind]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "\n",
    "        elif index//10000==1:\n",
    "            start=self.count_list[0] \n",
    "            ind=random.randint(start,start+self.count_list[index//10000]-1)\n",
    "            hr_image=self.trans(tf(Image.open(self.hr_images[ind]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "\n",
    "        elif index//10000==2:\n",
    "            start=self.count_list[0]+self.count_list[1]\n",
    "            ind=random.randint(start,start+self.count_list[index//10000]-1)\n",
    "            hr_image=self.trans(tf(Image.open(self.hr_images[ind]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "        elif index//10000==3:\n",
    "            start=self.count_list[0]+self.count_list[1]+self.count_list[2]\n",
    "            ind=random.randint(start,start+self.count_list[index//10000]-1)\n",
    "            hr_image=self.trans(tf(Image.open(self.hr_images[ind]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "\n",
    "        elif index//10000==4:\n",
    "            start=self.count_list[0]+self.count_list[1]+self.count_list[2]+self.count_list[3]\n",
    "            ind=random.randint(start,start+self.count_list[index//10000]-1)\n",
    "            hr_image=self.trans(tf(Image.open(self.hr_images[ind]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "\n",
    "        hr_image=hr_image*2-1    \n",
    "        lr_image = self.random_resample(hr_image, scale_factor=0.5)\n",
    "        \n",
    "        return lr_image, hr_image\n",
    "    \n",
    "image_path=[]\n",
    "count_list=[]\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list=glob(params['data_path']+class_list[i]+'/*.jpeg')\n",
    "    count_list.append(len(image_list))\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "\n",
    "train_dataset=SRDataset(params,image_path,count_list)\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, device, vgg_weight=0.006, l1_weight=1.0, adv_weight=0.001):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        \n",
    "        # L1 Loss\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "        # VGG19 for Perceptual Loss\n",
    "        vgg = models.vgg19(pretrained=True).features[:16].eval().to(device)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "\n",
    "        # Weights for each loss component\n",
    "        self.vgg_weight = vgg_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.adv_weight = adv_weight\n",
    "\n",
    "    def perceptual_loss(self, fake_img, real_img):\n",
    "        fake_features = self.vgg(fake_img)\n",
    "        real_features = self.vgg(real_img)\n",
    "        return F.l1_loss(fake_features, real_features)\n",
    "\n",
    "    def forward(self, fake_img, real_img, disc_fake_pred=None):\n",
    "        # L1 Loss\n",
    "        l1_loss = self.l1_loss(fake_img, real_img)\n",
    "        \n",
    "        # Perceptual Loss\n",
    "        perceptual_loss = self.perceptual_loss(fake_img, real_img)\n",
    "        \n",
    "        # Adversarial Loss (if provided)\n",
    "        if disc_fake_pred is not None:\n",
    "            adversarial_loss = F.softplus(-disc_fake_pred).mean()\n",
    "        else:\n",
    "            adversarial_loss = 0.0\n",
    "\n",
    "        # Combined loss with weights\n",
    "        total_loss = (\n",
    "            self.l1_weight * l1_loss +\n",
    "            self.vgg_weight * perceptual_loss +\n",
    "            self.adv_weight * adversarial_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "model = RealESRGAN(device, scale=2).model.to(device)\n",
    "criterion = CombinedLoss(device=device, vgg_weight=0.006, l1_weight=1.0, adv_weight=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "      Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "          Conv2d-1       [2, 64, 256, 256]           6,976           6,976\n",
      "            RRDB-2       [2, 64, 256, 256]         719,424         719,424\n",
      "            RRDB-3       [2, 64, 256, 256]         719,424         719,424\n",
      "            RRDB-4       [2, 64, 256, 256]         719,424         719,424\n",
      "            RRDB-5       [2, 64, 256, 256]         719,424         719,424\n",
      "            RRDB-6       [2, 64, 256, 256]         719,424         719,424\n",
      "            RRDB-7       [2, 64, 256, 256]         719,424         719,424\n",
      "            RRDB-8       [2, 64, 256, 256]         719,424         719,424\n",
      "            RRDB-9       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-10       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-11       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-12       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-13       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-14       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-15       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-16       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-17       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-18       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-19       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-20       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-21       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-22       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-23       [2, 64, 256, 256]         719,424         719,424\n",
      "           RRDB-24       [2, 64, 256, 256]         719,424         719,424\n",
      "         Conv2d-25       [2, 64, 256, 256]          36,928          36,928\n",
      "         Conv2d-26       [2, 64, 512, 512]          36,928          36,928\n",
      "      LeakyReLU-27       [2, 64, 512, 512]               0               0\n",
      "         Conv2d-28     [2, 64, 1024, 1024]          36,928          36,928\n",
      "         Conv2d-29     [2, 64, 1024, 1024]          36,928          36,928\n",
      "         Conv2d-30      [2, 3, 1024, 1024]           1,731           1,731\n",
      "===========================================================================\n",
      "Total params: 16,703,171\n",
      "Trainable params: 16,703,171\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'---------------------------------------------------------------------------\\n      Layer (type)            Output Shape         Param #     Tr. Param #\\n===========================================================================\\n          Conv2d-1       [2, 64, 256, 256]           6,976           6,976\\n            RRDB-2       [2, 64, 256, 256]         719,424         719,424\\n            RRDB-3       [2, 64, 256, 256]         719,424         719,424\\n            RRDB-4       [2, 64, 256, 256]         719,424         719,424\\n            RRDB-5       [2, 64, 256, 256]         719,424         719,424\\n            RRDB-6       [2, 64, 256, 256]         719,424         719,424\\n            RRDB-7       [2, 64, 256, 256]         719,424         719,424\\n            RRDB-8       [2, 64, 256, 256]         719,424         719,424\\n            RRDB-9       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-10       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-11       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-12       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-13       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-14       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-15       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-16       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-17       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-18       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-19       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-20       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-21       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-22       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-23       [2, 64, 256, 256]         719,424         719,424\\n           RRDB-24       [2, 64, 256, 256]         719,424         719,424\\n         Conv2d-25       [2, 64, 256, 256]          36,928          36,928\\n         Conv2d-26       [2, 64, 512, 512]          36,928          36,928\\n      LeakyReLU-27       [2, 64, 512, 512]               0               0\\n         Conv2d-28     [2, 64, 1024, 1024]          36,928          36,928\\n         Conv2d-29     [2, 64, 1024, 1024]          36,928          36,928\\n         Conv2d-30      [2, 3, 1024, 1024]           1,731           1,731\\n===========================================================================\\nTotal params: 16,703,171\\nTrainable params: 16,703,171\\nNon-trainable params: 0\\n---------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=torch.rand(2,3,512,512).to(device)\n",
    "tms.summary(model, img,show_input=False,print_summary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
