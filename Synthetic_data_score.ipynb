{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs used:\t8\n",
      "Device:\t\tcuda:4\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "from torchvision import datasets, transforms\n",
    "from pytorch_fid import fid_score\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy import linalg\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm \n",
    "from pytorch_fid.inception import InceptionV3\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.models import inception_v3\n",
    "import os\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",4)\n",
    "print(f\"Device:\\t\\t{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 735/735 [00:15<00:00, 47.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params={'image_size':299,\n",
    "        'data_path':'../../data/origin_type/STNT/유형1',\n",
    "        'batch_size':1,\n",
    "        }\n",
    "\n",
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images):\n",
    "        \n",
    "        self.images = images\n",
    "        self.args=parmas\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "image_list=glob(params['data_path']+'/*.jpeg')\n",
    "\n",
    "        \n",
    "train_images=torch.zeros((len(image_list),3,params['image_size'],params['image_size']))\n",
    "for i in tqdm(range(len(image_list))):\n",
    "    train_images[i]=trans(Image.open(image_list[i]).convert('RGB').resize((params['image_size'],params['image_size'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 735/735 [00:08<00:00, 88.18it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_images=torch.rand((200,3,299,299)).to(device)\n",
    "model = inception_v3(pretrained=True).to(device)\n",
    "model.eval()\n",
    "act=torch.zeros(len(train_images),1000)\n",
    "for i in tqdm(range(len(train_images))):\n",
    "    x=train_images[i].unsqueeze(0).to(device)\n",
    "    act[i]=model(x)[0].detach().cpu()\n",
    "act=act.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu1, sigma1=fid_score.calculate_activation_statistics(act[9:10])\n",
    "mu2, sigma2=fid_score.calculate_activation_statistics(act[9:10])\n",
    "fid=fid_score.calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6400)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
