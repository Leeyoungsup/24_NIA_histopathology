{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from glob import glob\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "tf=T.ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':16,\n",
    "        'epochs':500,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path='../../data/external/ori/*.png'\n",
    "mask1_path='../../data/external/mask/class1/*.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = Image.open(self.img_path[idx])\n",
    "        image_path=tf(image_path)\n",
    "        file_path=os.path.basename(self.img_path[idx])\n",
    "        label1 = np.array(Image.open(self.label[idx]))\n",
    "        label1=label1[:,:,0,np.newaxis]\n",
    "        label2=np.array(Image.open(self.label[idx].replace('/class1', '/class2')))\n",
    "        label2=label2[:,:,0,np.newaxis]\n",
    "        label3=np.array(Image.open(self.label[idx].replace('/class1', '/class3')))\n",
    "        label3=label3[:,:,0,np.newaxis]\n",
    "\n",
    "        label=np.concatenate((label1,label2,label3),axis=2)\n",
    "        label_path = tf(cv2.resize(label, (512, 512)))\n",
    "       \n",
    "        return image_path, label_path,file_path\n",
    "\n",
    "test_dataset = CustomDataset(glob(image_path), glob(mask1_path))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=params['batch_size'],shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, num_classes=3):\n",
    "    smooth = 1e-6\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "        \n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] =(2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return dice_per_class\n",
    "\n",
    "def compute_iou(pred_mask, true_mask, threshold=0.5, num_classes=3):\n",
    "    \"\"\"\n",
    "    IoU를 계산하는 함수\n",
    "\n",
    "    :param pred_mask: 모델이 예측한 마스크 (torch.Tensor)\n",
    "    :param true_mask: 실제 마스크 (torch.Tensor)\n",
    "    :param threshold: 이진화를 위한 임계값\n",
    "    :return: IoU 값\n",
    "    \"\"\"\n",
    "    iou_per_class = torch.zeros(num_classes).to(device)\n",
    "    for class_id in range(num_classes):\n",
    "    # 예측된 마스크 이진화\n",
    "        pred_mask1 = (pred_mask[:,class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 실제 마스크 이진화\n",
    "        true_mask1 = (true_mask[:,class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 교차 계산\n",
    "        intersection = torch.sum(pred_mask1 * true_mask1)\n",
    "        \n",
    "        # 합집합 계산\n",
    "        union = torch.sum(pred_mask1) + torch.sum(true_mask1) - intersection\n",
    "        \n",
    "        # IoU 계산\n",
    "        iou_per_class[class_id]= intersection / union\n",
    "    \n",
    "    return iou_per_class\n",
    "\n",
    "\n",
    "def compute_f1(pred_mask, true_mask, threshold=0.5, num_classes=3, device='cpu'):\n",
    "    \"\"\"\n",
    "    F1 점수를 계산하는 함수\n",
    "\n",
    "    :param pred_mask: 모델이 예측한 마스크 (torch.Tensor)\n",
    "    :param true_mask: 실제 마스크 (torch.Tensor)\n",
    "    :param threshold: 이진화를 위한 임계값\n",
    "    :param num_classes: 클래스의 수\n",
    "    :param device: 연산에 사용할 디바이스 (기본값: 'cpu')\n",
    "    :return: 각 클래스별 F1 점수, Precision, Recall, Specificity, Accuracy (torch.Tensor)\n",
    "    \"\"\"\n",
    "    f1_per_class = torch.zeros(num_classes).to(device)\n",
    "    precision1 = torch.zeros(num_classes).to(device)\n",
    "    recall1 = torch.zeros(num_classes).to(device)\n",
    "    specificity1 = torch.zeros(num_classes).to(device)\n",
    "    accuracy1 = torch.zeros(num_classes).to(device)\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # 예측된 마스크 이진화\n",
    "        pred_binary_mask = (pred_mask[:, class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 실제 마스크 이진화\n",
    "        true_binary_mask = (true_mask[:, class_id, ...] > threshold).float()\n",
    "        \n",
    "        # True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN) 계산\n",
    "        TP = torch.sum(pred_binary_mask * true_binary_mask)\n",
    "        FP = torch.sum(pred_binary_mask * (1 - true_binary_mask))\n",
    "        FN = torch.sum((1 - pred_binary_mask) * true_binary_mask)\n",
    "        TN = torch.sum((1 - pred_binary_mask) * (1 - true_binary_mask))\n",
    "        \n",
    "        # 정밀도 (Precision) 계산\n",
    "        precision = TP / (TP + FP + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 재현율 (Recall) 계산\n",
    "        recall = TP / (TP + FN + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 특이도 (Specificity) 계산\n",
    "        specificity = TN / (TN + FP + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 정확도 (Accuracy) 계산\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # F1 점수 계산\n",
    "        f1_per_class[class_id] = 2 * (precision * recall) / (precision + recall + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        precision1[class_id] = precision.item()\n",
    "        recall1[class_id] = recall.item()\n",
    "        specificity1[class_id] = specificity.item()\n",
    "        accuracy1[class_id] = accuracy.item()\n",
    "    \n",
    "    return f1_per_class, precision1, recall1, specificity1, accuracy1\n",
    "\n",
    "class MultiClassIoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(MultiClassIoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets,num_classes=3, smooth=1):\n",
    "        num_classes = inputs.shape[1]  # assuming inputs have shape (batch_size, num_classes, height, width)\n",
    "        inputs = F.softmax(inputs, dim=1)  # apply softmax to the input tensor\n",
    "        IOU_per_class = torch.zeros(num_classes).to(device)\n",
    "        for cls in range(num_classes):\n",
    "            input_cls = inputs[:, cls, :, :].contiguous().view(-1)  # flatten the inputs for the current class\n",
    "            target_cls = targets[:, cls, :, :].float().contiguous().view(-1)  # create binary target for the current class\n",
    "\n",
    "            intersection = (input_cls * target_cls).sum()\n",
    "            total = (input_cls + target_cls).sum()\n",
    "            union = total - intersection\n",
    "\n",
    "            IoU = (intersection + smooth) / (union + smooth)\n",
    "            IOU_per_class[cls] = (1 - IoU)\n",
    "\n",
    "        return IOU_per_class # average over all classes\n",
    "    \n",
    "model = smp.MAnet(\n",
    "    encoder_name=\"efficientnet-b5\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=3,                      # model output channels (number of classes in your dataset)\n",
    ").to(device)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], betas=(params['beta1'], params['beta2']))\n",
    "criterion = MultiClassIoULoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToPILImage()\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load('../../model/MANet/MANet_'+str(i+1)+'_check.pth',map_location=device))\n",
    "    model.to(device)\n",
    "    df=pd.DataFrame(columns=['file_name','Dice1','Dice2','Dice3','mDice','IoU1','IoU2','IoU3','mIoU','f1','precision','sensitivity','specificity','accuracy'])\n",
    "    with torch.no_grad():\n",
    "        test = tqdm(test_dataloader)\n",
    "        count = 0\n",
    "        val_running_loss = 0.0\n",
    "        acc_loss = 0\n",
    "        for x, y,file_path in test:\n",
    "            model.eval()\n",
    "            y = y.to(device).float()\n",
    "            count += 1\n",
    "            x = x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = dice_loss(predict,y)  # cost 구함\n",
    "            iou=compute_iou(predict, y)\n",
    "            f1,precision,recall,specificity, accuracy=compute_f1(predict, y)\n",
    "            val_running_loss+=cost.mean().item()\n",
    "            df.loc[len(df)]=[file_path[0],cost[0].item(),cost[1].item(),cost[2].item(),cost.mean().item(),iou[0].item(),iou[1].item(),iou[2].item(),iou.mean().item(),f1.mean().item(),precision.mean().item(),recall.mean().item(),specificity.mean().item(),accuracy.mean().item()]\n",
    "            transform(y[0].cpu()).save('../../data/external/result/MANet/k_'+str(i+1)+'/label/'+file_path[0])\n",
    "            transform(torch.where(predict[0]>0.5,1,0).cpu().float()).save('../../data/external/result/MANet/k_'+str(i+1)+'/pred/'+file_path[0])\n",
    "            test.set_description(\n",
    "                f\"val_Step: {count+1} dice_sore : {val_running_loss/count:.4f}\")\n",
    "    df.to_csv('../../data/external/result/MANet/MANet_'+str(i+1)+'_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1=np.load('../../data/cv0_ori.npy')\n",
    "image1=image1.astype(np.uint8)\n",
    "image2=np.load('../../data/cv1_ori.npy')\n",
    "image2=image2.astype(np.uint8)\n",
    "image3=np.load('../../data/cv2_ori.npy')\n",
    "image3=image3.astype(np.uint8)\n",
    "image4=np.load('../../data/cv3_ori.npy')\n",
    "image4=image4.astype(np.uint8)\n",
    "image5=np.load('../../data/cv4_ori.npy')\n",
    "image5=image5.astype(np.uint8)\n",
    "mask1=np.load('../../data/cv0_mask.npy')\n",
    "mask1=(mask1[:,:,:,:3]).astype(np.uint8)\n",
    "mask2=np.load('../../data/cv1_mask.npy')\n",
    "mask2=(mask2[:,:,:,:3]).astype(np.uint8)\n",
    "mask3=np.load('../../data/cv2_mask.npy')\n",
    "mask3=(mask3[:,:,:,:3]).astype(np.uint8)\n",
    "mask4=np.load('../../data/cv3_mask.npy')\n",
    "mask4=(mask4[:,:,:,:3]).astype(np.uint8)\n",
    "mask5=np.load('../../data/cv4_mask.npy')\n",
    "mask5=(mask5[:,:,:,:3]).astype(np.uint8)\n",
    "name1=np.load('../../data/cv0_name.npy')\n",
    "name2=np.load('../../data/cv1_name.npy')\n",
    "name3=np.load('../../data/cv2_name.npy')\n",
    "name4=np.load('../../data/cv3_name.npy')\n",
    "name5=np.load('../../data/cv4_name.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data={'image1':image1,'image2':image2,'image3':image3,'image4':image4,'image5':image5,'mask1':mask1,'mask2':mask2,'mask3':mask3,'mask4':mask4,'mask5':mask5,'name1':name1,'name2':name2,'name3':name3,'name4':name4,'name5':name5}\n",
    "def dice_loss(pred, target, num_classes=3):\n",
    "    smooth = 1e-6\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "        \n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] =(2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return dice_per_class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list,name_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "        self.name=name_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_path[idx]\n",
    "        image_path=tf(cv2.cvtColor(image_path, cv2.COLOR_GRAY2RGB))\n",
    "        \n",
    "        label_path = self.label[idx]\n",
    "        label_path = tf(cv2.resize(label_path, (512, 512)))\n",
    "        \n",
    "        name=self.name[idx]\n",
    "       \n",
    "        return image_path, label_path,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_Step: 9763 dice_sore : 0.8356: 100%|██████████| 9762/9762 [13:19<00:00, 12.21it/s]\n",
      "test_Step: 10007 dice_sore : 0.8446: 100%|██████████| 10006/10006 [14:23<00:00, 11.59it/s]\n",
      "test_Step: 10093 dice_sore : 0.8424: 100%|██████████| 10092/10092 [14:25<00:00, 11.66it/s]\n",
      "test_Step: 10185 dice_sore : 0.8467: 100%|██████████| 10184/10184 [14:58<00:00, 11.33it/s]\n",
      "test_Step: 9601 dice_sore : 0.8532: 100%|██████████| 9600/9600 [16:06<00:00,  9.94it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = T.ToPILImage()\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load('../../model/MANet/MANet_'+str(i+1)+'_check.pth',map_location=device))\n",
    "    model.to(device)\n",
    "    test_image=np_data['image'+str(i+1)]\n",
    "    test_mask=np_data['mask'+str(i+1)]\n",
    "    test_name=np_data['name'+str(i+1)]\n",
    "    test_dataset = CustomDataset(test_image, test_mask,test_name)\n",
    "    test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "    df=pd.DataFrame(columns=['file_name','dice1','dice2','dice3','mDice'])\n",
    "    with torch.no_grad():\n",
    "        test = tqdm(test_dataloader)\n",
    "        count = 0\n",
    "        test_running_loss = 0.0\n",
    "        acc_loss = 0\n",
    "        for x, y,file_path in test:\n",
    "            y = y.to(device).float()\n",
    "            count += 1\n",
    "            x = x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = dice_loss(predict,y)  # cost 구함\n",
    "            iou=compute_iou(predict, y)\n",
    "            f1,precision,recall,specificity, accuracy=compute_f1(predict, y)\n",
    "            test_running_loss+=cost.mean().item()\n",
    "            df.loc[len(df)]=[file_path[0]+'.png',cost[0].item(),cost[1].item(),cost[2].item(),cost.mean().item()]\n",
    "            transform(y[0].cpu()).save('../../data/internal/result/MANet/k_'+str(i+1)+'/label/'+file_path[0]+'.png')\n",
    "            transform(torch.where(predict[0]>0.5,1,0).cpu().float()).save('../../data/internal/result/MANet/k_'+str(i+1)+'/pred/'+file_path[0]+'.png')\n",
    "            test.set_description(\n",
    "                f\"test_Step: {count+1} dice_sore : {test_running_loss/count:.4f}\")\n",
    "    df.to_csv('../../data/internal/result/MANet/MANet_'+str(i+1)+'_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `target` has to be an integer tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/metric.py:303\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/metric.py:372\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/metric.py:465\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/classification/dice.py:209\u001b[0m, in \u001b[0;36mDice.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m \u001b[43m_stat_scores_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmdmc_reduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdmc_reduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Update states\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce \u001b[38;5;241m!=\u001b[39m AverageMethod\u001b[38;5;241m.\u001b[39mSAMPLES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmdmc_reduce \u001b[38;5;241m!=\u001b[39m MDMCAverageMethod\u001b[38;5;241m.\u001b[39mSAMPLEWISE:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/functional/classification/stat_scores.py:956\u001b[0m, in \u001b[0;36m_stat_scores_update\u001b[0;34m(preds, target, reduce, mdmc_reduce, num_classes, top_k, threshold, multiclass, ignore_index, mode)\u001b[0m\n\u001b[1;32m    953\u001b[0m     preds, target \u001b[38;5;241m=\u001b[39m _drop_negative_ignored_indices(preds, target, ignore_index, mode)\n\u001b[1;32m    954\u001b[0m     _negative_index_dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m preds, target, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_input_format_classification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ignore_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `ignore_index` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mignore_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid for inputs with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/utilities/checks.py:414\u001b[0m, in \u001b[0;36m_input_format_classification\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m    412\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 414\u001b[0m case \u001b[38;5;241m=\u001b[39m \u001b[43m_check_classification_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m (DataType\u001b[38;5;241m.\u001b[39mBINARY, DataType\u001b[38;5;241m.\u001b[39mMULTILABEL) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m top_k:\n\u001b[1;32m    425\u001b[0m     preds \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mint()\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/utilities/checks.py:270\u001b[0m, in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k, ignore_index)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform error checking on inputs for classification.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03mThis ensures that preds and target take one of the shape/type combinations that are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Basic validation (that does not need case/type information)\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43m_basic_input_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Check that shape/types fall into one of the cases\u001b[39;00m\n\u001b[1;32m    273\u001b[0m case, implied_classes \u001b[38;5;241m=\u001b[39m _check_shape_and_type_consistency(preds, target)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchmetrics/utilities/checks.py:56\u001b[0m, in \u001b[0;36m_basic_input_validation\u001b[0;34m(preds, target, threshold, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `target` has to be an integer tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (ignore_index \u001b[38;5;129;01mand\u001b[39;00m ignore_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m target\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `target` has to be a non-negative tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The `target` has to be an integer tensor."
     ]
    }
   ],
   "source": [
    "dice(predict,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses.dice import DiceLoss, one_hot\n",
    "dice = DiceLoss(reduction='mean')\n",
    "dice(predict,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg1 =  predict[:,0 ]\n",
    "gt1 = y[: ,0]\n",
    "seg1_n = seg1 < 0.5\n",
    "seg1_t = seg1 >= 0.5\n",
    "\n",
    "gt1_n = gt1 == 0\n",
    "gt1_t = gt1 == 1\n",
    "tp = torch.sum(seg1_t&gt1_t)\n",
    "\n",
    "fp = torch.sum(seg1_t&gt1_n)\n",
    "\n",
    "tn = torch.sum(seg1_n&gt1_n)\n",
    "\n",
    "fn = torch.sum(gt1_t&seg1_n)\n",
    "dice = (2*tp) / (2*tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = torch.sum(seg1_t[0]&gt1_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
