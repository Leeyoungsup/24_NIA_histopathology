{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/onnx/_internal/_beartype.py:36: UserWarning: unhashable type: 'list'\n",
      "  warnings.warn(f\"{e}\")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import time\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import os\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm import create_model\n",
    "import cv2\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=1\n",
    "img_size=1024\n",
    "class_list=['NT_epithelial','NT_immune','NT_stroma','TP_in_situ','TP_invasive']\n",
    "class_weight=[2,0,1,3,4]\n",
    "tf = ToTensor()\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:06<00:00, 12.98it/s]\n"
     ]
    }
   ],
   "source": [
    "img_path='../../data/area_segmentation/BR_class/image/'\n",
    "img_list=glob(img_path+'NIA6_R_BRID*.jpeg')\n",
    "mask_list=[]\n",
    "train_img_list,test_img_list=train_test_split(img_list,test_size=0.2,random_state=42)\n",
    "test_image=torch.zeros((len(test_img_list),3,img_size,img_size))\n",
    "test_mask=torch.zeros((len(test_img_list),len(class_list),img_size,img_size),dtype=torch.float32)    \n",
    "\n",
    "for i in tqdm(range(len(test_img_list))):\n",
    "    test_image[i] = tf(Image.open(test_img_list[i]))\n",
    "    for j in range(len(class_list)):\n",
    "        tf_mask=tf(Image.open(test_img_list[i].replace('/image', '/'+class_list[j])).convert('L'))\n",
    "        test_mask[i,j]=tf_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "        \n",
    "    def trans(self,image,label):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            label = transform(label)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            label = transform(label)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path,label_path = self.img_path[idx],self.label[idx]\n",
    "        \n",
    "        return image_path, label_path\n",
    "\n",
    "test_dataset = CustomDataset(test_image, test_mask)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_649088/4153989047.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  NT_epithelial_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[0]+'.pt',map_location=device))\n",
      "/tmp/ipykernel_649088/4153989047.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  NT_immune_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[1]+'.pt',map_location=device))\n",
      "/tmp/ipykernel_649088/4153989047.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  NT_stroma_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[2]+'.pt',map_location=device))\n",
      "/tmp/ipykernel_649088/4153989047.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  TP_in_situ_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[3]+'.pt',map_location=device))\n",
      "/tmp/ipykernel_649088/4153989047.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  TP_invasive_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[4]+'.pt',map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NT_epithelial_model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2,                      # model output channels (number of classes in your dataset) \n",
    "    ).to(device)\n",
    "NT_immune_model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2,                      # model output channels (number of classes in your dataset) \n",
    "    ).to(device)\n",
    "NT_stroma_model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2,                      # model output channels (number of classes in your dataset) \n",
    "    ).to(device)\n",
    "TP_in_situ_model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2,                      # model output channels (number of classes in your dataset) \n",
    "    ).to(device)\n",
    "TP_invasive_model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2,                      # model output channels (number of classes in your dataset) \n",
    "    ).to(device)\n",
    "def dice_loss(pred, target, num_classes=2):\n",
    "    smooth = 1e-6\n",
    "    dice_per_class = torch.zeros((len(pred),num_classes)).to(pred.device)\n",
    "    pred=F.softmax(pred,dim=1)\n",
    "    for i in range(len(pred)):\n",
    "        for class_id in range(num_classes):\n",
    "            pred_class = pred[i, class_id, ...]\n",
    "            target_class = target[i, class_id, ...]\n",
    "            \n",
    "            intersection = torch.sum(pred_class * target_class)\n",
    "            A_sum = torch.sum(pred_class * pred_class)\n",
    "            B_sum = torch.sum(target_class * target_class)\n",
    "            dice_per_class[i,class_id] =(2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return 1-dice_per_class.mean()\n",
    "# summary(model,(batch_size,3,img_size,img_size))\n",
    "\n",
    "NT_epithelial_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[0]+'.pt',map_location=device))\n",
    "NT_immune_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[1]+'.pt',map_location=device))\n",
    "NT_stroma_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[2]+'.pt',map_location=device))\n",
    "TP_in_situ_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[3]+'.pt',map_location=device))\n",
    "TP_invasive_model.load_state_dict(torch.load('../../model/areaSeg/BR_'+class_list[4]+'.pt',map_location=device))\n",
    "model_list=[NT_epithelial_model,NT_immune_model,NT_stroma_model,TP_in_situ_model,TP_invasive_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:6')\n",
      "tensor(0, device='cuda:6')\n",
      "tensor(0, device='cuda:6')\n",
      "tensor(0, device='cuda:6')\n",
      "tensor(0, device='cuda:6')\n",
      "tensor(0, device='cuda:6')\n",
      "tensor(0, device='cuda:6')\n",
      "tensor(0, device='cuda:6')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m predict_index\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mwhere(F\u001b[38;5;241m.\u001b[39msoftmax(predict1, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     predict[\u001b[38;5;241m0\u001b[39m,j][predict_index]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     28\u001b[0m predict[\u001b[38;5;241m0\u001b[39m,i][predict_index]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m4\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "k = cv2.getStructuringElement(cv2.MORPH_RECT, (7,7))\n",
    "train_loss_list=[]\n",
    "val_loss_list=np.zeros((len(test_dataloader),5))\n",
    "train_acc_list=[]\n",
    "val_acc_list=[]\n",
    "MIN_loss=5000\n",
    "metrics = defaultdict(float)\n",
    "for i in range(len(model_list)):\n",
    "    model_list[i].eval()\n",
    "count=0\n",
    "val_running_loss=0.0\n",
    "acc_loss=0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dataloader:\n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        predict=torch.zeros((1,5,1024,1024)).to(device)\n",
    "        for i in class_weight:\n",
    "            model=model_list[i]\n",
    "            predict1 = model(x).to(device)\n",
    "            cost = dice_loss(predict1, y[:,i]).cpu().numpy()\n",
    "            val_loss_list[count-1,i]=1-cost\n",
    "            predict_index=torch.where(F.softmax(predict1, dim=1).argmax(dim=1)==1)[0]\n",
    "            for j in range(5):\n",
    "                predict[0,j][predict_index]=0.\n",
    "            predict[0,i][predict_index]=1.\n",
    "            if i==4:\n",
    "                print(F.softmax(predict1, dim=1).argmax(dim=1).max())\n",
    "        y = y.to('cpu')\n",
    "        x=x.to('cpu')\n",
    "        \n",
    "        if count%20==0:\n",
    "            pred_softmax=np.array(predict.cpu())\n",
    "            mask=np.zeros((img_size,img_size,3))\n",
    "            mask[...,0]=cv2.morphologyEx(pred_softmax[0][0], cv2.MORPH_OPEN, k)\n",
    "            mask[...,1]=cv2.morphologyEx(pred_softmax[0][1], cv2.MORPH_OPEN, k)\n",
    "            mask[...,2]=cv2.morphologyEx(pred_softmax[0][2], cv2.MORPH_OPEN, k)\n",
    "            mask[...,1]+=cv2.morphologyEx(pred_softmax[0][3], cv2.MORPH_OPEN, k)\n",
    "            mask[...,2]+=cv2.morphologyEx(pred_softmax[0][3], cv2.MORPH_OPEN, k)\n",
    "            mask[...,0]+=cv2.morphologyEx(pred_softmax[0][4], cv2.MORPH_OPEN, k)\n",
    "            mask[...,1]+=cv2.morphologyEx(pred_softmax[0][4], cv2.MORPH_OPEN, k)\n",
    "            label=np.zeros((img_size,img_size,3))\n",
    "            label[...,0]=np.array(y[0][0])\n",
    "            label[...,1]=np.array(y[0][1])\n",
    "            label[...,2]=np.array(y[0][2])\n",
    "            label[...,1]+=np.array(y[0][3])\n",
    "            label[...,2]+=np.array(y[0][3])\n",
    "            label[...,0]+=np.array(y[0][4])\n",
    "            label[...,1]+=np.array(y[0][4])\n",
    "            img=topilimage(x[0])\n",
    "            overlay=np.array(img)*0.7+np.array(mask)*0.3\n",
    "            overlay=Image.fromarray(overlay.astype('uint8'))\n",
    "            fig = plt.figure(figsize=(20,30))\n",
    "            rows = 1\n",
    "            cols = 4\n",
    "            ax1 = fig.add_subplot(rows, cols, 1)\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title('image')\n",
    "            ax1.axis(\"off\")\n",
    "            \n",
    "            ax2 = fig.add_subplot(rows, cols, 2)\n",
    "            ax2.imshow(label)\n",
    "            ax2.set_title('label')\n",
    "            ax2.axis(\"off\")\n",
    "            \n",
    "            ax3 = fig.add_subplot(rows, cols, 3)\n",
    "            ax3.imshow(mask)\n",
    "            ax3.set_title('predict')\n",
    "            ax3.axis(\"off\")\n",
    "            \n",
    "            ax4 = fig.add_subplot(rows, cols, 4)\n",
    "            ax4.imshow(overlay)\n",
    "            ax4.set_title('overlay')\n",
    "            ax4.axis(\"off\")\n",
    "            plt.show()\n",
    "           \n",
    "\n",
    "print('batch size= 4')\n",
    "print('image size= 224')\n",
    "print('learning rate= 0.0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/contours.cpp:195: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m poly\u001b[38;5;241m=\u001b[39m\u001b[43mbinary_mask_to_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m polygon_arrays \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polygon \u001b[38;5;129;01min\u001b[39;00m poly\u001b[38;5;241m.\u001b[39mgeoms:\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mbinary_mask_to_polygon\u001b[0;34m(binary_mask)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinary_mask_to_polygon\u001b[39m(binary_mask):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# binary_mask는 2차원 numpy array여야 합니다.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Contours를 찾습니다.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     contours, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindContours\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRETR_EXTERNAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHAIN_APPROX_SIMPLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     polygons \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m contour \u001b[38;5;129;01min\u001b[39;00m contours:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# 각 contour를 polygon으로 변환\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/contours.cpp:195: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import cv2\n",
    "def binary_mask_to_polygon(binary_mask):\n",
    "    # binary_mask는 2차원 numpy array여야 합니다.\n",
    "    # Contours를 찾습니다.\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        # 각 contour를 polygon으로 변환\n",
    "        if len(contour) >= 3:  # 유효한 polygon을 만들기 위해서 최소한 3개의 점이 필요합니다.\n",
    "            poly = Polygon(shell=[(point[0][0], point[0][1]) for point in contour])\n",
    "            polygons.append(poly)\n",
    "    \n",
    "    if len(polygons) > 1:\n",
    "        # 여러 개의 polygon이 있을 경우 MultiPolygon으로 변환\n",
    "        return polygons\n",
    "    elif len(polygons) == 1:\n",
    "        return polygons[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "poly=binary_mask_to_polygon(np.array(mask)[...,1])\n",
    "polygon_arrays = []\n",
    "for polygon in poly.geoms:\n",
    "    exterior_coords = np.array(polygon.exterior.coords)\n",
    "    polygon_arrays.append(exterior_coords)\n",
    "\n",
    "# 변환된 numpy 배열 출력\n",
    "for i, array in enumerate(polygon_arrays):\n",
    "    print(f\"Polygon {i+1} coordinates:\")\n",
    "    print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for polygon in poly:\n",
    "    print(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.draw import polygon2mask\n",
    "mask1=np.zeros((1024,1024))\n",
    "for i in range(len(polygon_arrays)):S\n",
    "    coordinate=np.array(polygon_arrays[i])\n",
    "    coordinate[:,[0, 1]]=coordinate[:,[1, 0]]\n",
    "    mask_temp=polygon2mask((1024,1024), coordinate)\n",
    "    mask1+=mask_temp\n",
    "ax=plt.figure(figsize=(10,10))\n",
    "ax.add_subplot(1,3,1)\n",
    "plt.imshow(img)\n",
    "ax.add_subplot(1,3,2)\n",
    "plt.imshow(mask1)\n",
    "ax.add_subplot(1,3,3)\n",
    "img1=np.array(img)\n",
    "img1[...,0]=img1[...,0]*0.8+mask1*255*0.2\n",
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(polygon_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
