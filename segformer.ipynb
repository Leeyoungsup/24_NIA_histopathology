{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "tf=T.ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-3,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':500,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path='../../data/external/ori/*.png'\n",
    "mask1_path='../../data/external/mask/class1/*.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = Image.open(self.img_path[idx])\n",
    "        image_path=tf(image_path)\n",
    "        file_path=os.path.basename(self.img_path[idx])\n",
    "        label1 = np.array(Image.open(self.label[idx]))\n",
    "        label1=label1[:,:,0,np.newaxis]\n",
    "        label2=np.array(Image.open(self.label[idx].replace('/class1', '/class2')))\n",
    "        label2=label2[:,:,0,np.newaxis]\n",
    "        label3=np.array(Image.open(self.label[idx].replace('/class1', '/class3')))\n",
    "        label3=label3[:,:,0,np.newaxis]\n",
    "\n",
    "        label=np.concatenate((label1,label2,label3),axis=2)\n",
    "        label_path = tf(cv2.resize(label, (512, 512)))\n",
    "       \n",
    "        return image_path, label_path,file_path\n",
    "\n",
    "test_dataset = CustomDataset(glob(image_path), glob(mask1_path))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=params['batch_size'],shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, num_classes=3):\n",
    "    smooth = 1e-6\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] =(2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return dice_per_class\n",
    "\n",
    "def compute_iou(pred_mask, true_mask, threshold=0.5, num_classes=3):\n",
    "    \"\"\"\n",
    "    IoU를 계산하는 함수\n",
    "\n",
    "    :param pred_mask: 모델이 예측한 마스크 (torch.Tensor)\n",
    "    :param true_mask: 실제 마스크 (torch.Tensor)\n",
    "    :param threshold: 이진화를 위한 임계값\n",
    "    :return: IoU 값\n",
    "    \"\"\"\n",
    "    iou_per_class = torch.zeros(num_classes).to(device)\n",
    "    for class_id in range(num_classes):\n",
    "    # 예측된 마스크 이진화\n",
    "        pred_mask1 = (pred_mask[:,class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 실제 마스크 이진화\n",
    "        true_mask1 = (true_mask[:,class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 교차 계산\n",
    "        intersection = torch.sum(pred_mask1 * true_mask1)\n",
    "        \n",
    "        # 합집합 계산\n",
    "        union = torch.sum(pred_mask1) + torch.sum(true_mask1) - intersection\n",
    "        \n",
    "        # IoU 계산\n",
    "        iou_per_class[class_id]= intersection / union\n",
    "    \n",
    "    return iou_per_class\n",
    "\n",
    "def compute_f1(pred_mask, true_mask, threshold=0.5, num_classes=3, device='cpu'):\n",
    "    \"\"\"\n",
    "    F1 점수를 계산하는 함수\n",
    "\n",
    "    :param pred_mask: 모델이 예측한 마스크 (torch.Tensor)\n",
    "    :param true_mask: 실제 마스크 (torch.Tensor)\n",
    "    :param threshold: 이진화를 위한 임계값\n",
    "    :param num_classes: 클래스의 수\n",
    "    :param device: 연산에 사용할 디바이스 (기본값: 'cpu')\n",
    "    :return: 각 클래스별 F1 점수, Precision, Recall, Specificity, Accuracy (torch.Tensor)\n",
    "    \"\"\"\n",
    "    f1_per_class = torch.zeros(num_classes).to(device)\n",
    "    precision1 = torch.zeros(num_classes).to(device)\n",
    "    recall1 = torch.zeros(num_classes).to(device)\n",
    "    specificity1 = torch.zeros(num_classes).to(device)\n",
    "    accuracy1 = torch.zeros(num_classes).to(device)\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # 예측된 마스크 이진화\n",
    "        pred_binary_mask = (pred_mask[:, class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 실제 마스크 이진화\n",
    "        true_binary_mask = (true_mask[:, class_id, ...] > threshold).float()\n",
    "        \n",
    "        # True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN) 계산\n",
    "        TP = torch.sum(pred_binary_mask * true_binary_mask)\n",
    "        FP = torch.sum(pred_binary_mask * (1 - true_binary_mask))\n",
    "        FN = torch.sum((1 - pred_binary_mask) * true_binary_mask)\n",
    "        TN = torch.sum((1 - pred_binary_mask) * (1 - true_binary_mask))\n",
    "        \n",
    "        # 정밀도 (Precision) 계산\n",
    "        precision = TP / (TP + FP + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 재현율 (Recall) 계산\n",
    "        recall = TP / (TP + FN + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 특이도 (Specificity) 계산\n",
    "        specificity = TN / (TN + FP + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 정확도 (Accuracy) 계산\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # F1 점수 계산\n",
    "        f1_per_class[class_id] = 2 * (precision * recall) / (precision + recall + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        precision1[class_id] = precision.item()\n",
    "        recall1[class_id] = recall.item()\n",
    "        specificity1[class_id] = specificity.item()\n",
    "        accuracy1[class_id] = accuracy.item()\n",
    "    \n",
    "    return f1_per_class, precision1, recall1, specificity1, accuracy1\n",
    "\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"matei-dorian/segformer-b5-finetuned-human-parsing\",num_labels=3,ignore_mismatched_sizes=True).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], betas=(params['beta1'], params['beta2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToPILImage()\n",
    "\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load('../../model/segformer/seg_former_'+str(i+1)+'_check.pth',map_location=device))\n",
    "    df=pd.DataFrame(columns=['file_name','Dice1','Dice2','Dice3','mDice','IoU1','IoU2','IoU3','mIoU','f1','precision','sensitivity','specificity','accuracy'])\n",
    "    with torch.no_grad():\n",
    "        test = tqdm(test_dataloader)\n",
    "        count = 0\n",
    "        val_running_loss = 0.0\n",
    "        acc_loss = 0\n",
    "        for x, y,file_path in test:\n",
    "            model.eval()\n",
    "            y = y.to(device).float()\n",
    "            count += 1\n",
    "            x = x.to(device).float()\n",
    "            output =model(x).logits.cpu()\n",
    "            predict = nn.functional.interpolate(\n",
    "                    output,\n",
    "                    size=(512,512),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "            ).to(device)\n",
    "            cost = dice_loss(predict,y)  # cost 구함\n",
    "            iou=compute_iou(predict, y)\n",
    "            f1,precision,recall,specificity, accuracy=compute_f1(predict, y)\n",
    "            val_running_loss+=cost.mean().item()\n",
    "            df.loc[len(df)]=[file_path[0],cost[0].item(),cost[1].item(),cost[2].item(),cost.mean().item(),iou[0].item(),iou[1].item(),iou[2].item(),iou.mean().item(),f1.mean().item(),precision.mean().item(),recall.mean().item(),specificity.mean().item(),accuracy.mean().item()]\n",
    "            transform(y[0].cpu()).save('../../data/external/result/segformer/k_'+str(i+1)+'/label/'+file_path[0])\n",
    "            transform(torch.where(predict[0]>0.5,1,0).cpu().float()).save('../../data/external/result/segformer/k_'+str(i+1)+'/pred/'+file_path[0])\n",
    "            test.set_description(\n",
    "                f\"val_Step: {count+1} dice_sore : {val_running_loss/count:.4f}\")\n",
    "    df.to_csv('../../data/external/result/segformer/segformer_'+str(i+1)+'_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1=np.load('../../data/cv0_ori.npy')\n",
    "image1=image1.astype(np.uint8)\n",
    "image2=np.load('../../data/cv1_ori.npy')\n",
    "image2=image2.astype(np.uint8)\n",
    "image3=np.load('../../data/cv2_ori.npy')\n",
    "image3=image3.astype(np.uint8)\n",
    "image4=np.load('../../data/cv3_ori.npy')\n",
    "image4=image4.astype(np.uint8)\n",
    "image5=np.load('../../data/cv4_ori.npy')\n",
    "image5=image5.astype(np.uint8)\n",
    "mask1=np.load('../../data/cv0_mask.npy')\n",
    "mask1=(mask1[:,:,:,:3]).astype(np.uint8)\n",
    "mask2=np.load('../../data/cv1_mask.npy')\n",
    "mask2=(mask2[:,:,:,:3]).astype(np.uint8)\n",
    "mask3=np.load('../../data/cv2_mask.npy')\n",
    "mask3=(mask3[:,:,:,:3]).astype(np.uint8)\n",
    "mask4=np.load('../../data/cv3_mask.npy')\n",
    "mask4=(mask4[:,:,:,:3]).astype(np.uint8)\n",
    "mask5=np.load('../../data/cv4_mask.npy')\n",
    "mask5=(mask5[:,:,:,:3]).astype(np.uint8)\n",
    "name1=np.load('../../data/cv0_name.npy')\n",
    "name2=np.load('../../data/cv1_name.npy')\n",
    "name3=np.load('../../data/cv2_name.npy')\n",
    "name4=np.load('../../data/cv3_name.npy')\n",
    "name5=np.load('../../data/cv4_name.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data={'image1':image1,'image2':image2,'image3':image3,'image4':image4,'image5':image5,'mask1':mask1,'mask2':mask2,'mask3':mask3,'mask4':mask4,'mask5':mask5,'name1':name1,'name2':name2,'name3':name3,'name4':name4,'name5':name5}\n",
    "def dice_loss(pred, target, num_classes=3):\n",
    "    smooth = 1e-6\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "        \n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] =(2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return dice_per_class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list,name_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "        self.name=name_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_path[idx]\n",
    "        image_path=tf(cv2.cvtColor(image_path, cv2.COLOR_GRAY2RGB))\n",
    "        \n",
    "        label_path = self.label[idx]\n",
    "        label_path = tf(cv2.resize(label_path[:,:,:3], (512, 512)))\n",
    "        \n",
    "        name=self.name[idx]\n",
    "       \n",
    "        return image_path, label_path,name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_Step: 9763 dice_sore : 0.9131: 100%|██████████| 9762/9762 [15:22<00:00, 10.58it/s]\n",
      "val_Step: 10007 dice_sore : 0.9277: 100%|██████████| 10006/10006 [17:47<00:00,  9.37it/s]\n",
      "val_Step: 10093 dice_sore : 0.9194: 100%|██████████| 10092/10092 [18:25<00:00,  9.13it/s]\n",
      "val_Step: 10185 dice_sore : 0.9263: 100%|██████████| 10184/10184 [19:02<00:00,  8.91it/s]\n",
      "val_Step: 9601 dice_sore : 0.9293: 100%|██████████| 9600/9600 [19:11<00:00,  8.34it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = T.ToPILImage()\n",
    "\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load('../../model/segformer/seg_former_'+str(i+1)+'_check.pth',map_location=device))\n",
    "    model.to(device)\n",
    "    test_image=np_data['image'+str(i+1)]\n",
    "    test_mask=np_data['mask'+str(i+1)]\n",
    "    test_name=np_data['name'+str(i+1)]\n",
    "    test_dataset = CustomDataset(test_image, test_mask,test_name)\n",
    "    test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "    df=pd.DataFrame(columns=['file_name','Dice1','Dice2','Dice3','mDice','IoU1','IoU2','IoU3','mIoU','f1','precision','sensitivity','specificity','accuracy'])\n",
    "    with torch.no_grad():\n",
    "        test = tqdm(test_dataloader)\n",
    "        count = 0\n",
    "        val_running_loss = 0.0\n",
    "        acc_loss = 0\n",
    "        for x, y,file_path in test:\n",
    "            model.eval()\n",
    "            y = y.to(device).float()\n",
    "            count += 1\n",
    "            x = x.to(device).float()\n",
    "            output =model(x).logits.cpu()\n",
    "            predict = nn.functional.interpolate(\n",
    "                    output,\n",
    "                    size=(512,512),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "            ).to(device)\n",
    "            cost = dice_loss(predict,y)  # cost 구함\n",
    "            iou=compute_iou(predict, y)\n",
    "            f1,precision,recall,specificity, accuracy=compute_f1(predict, y)\n",
    "            val_running_loss+=cost.mean().item()\n",
    "            df.loc[len(df)]=[file_path[0]+'.png',cost[0].item(),cost[1].item(),cost[2].item(),cost.mean().item(),iou[0].item(),iou[1].item(),iou[2].item(),iou.mean().item(),f1.mean().item(),precision.mean().item(),recall.mean().item(),specificity.mean().item(),accuracy.mean().item()]\n",
    "            transform(y[0].cpu()).save('../../data/internal/result/segformer/k_'+str(i+1)+'/label/'+file_path[0]+'.png')\n",
    "            transform(torch.where(predict[0]>0.5,1,0).cpu().float()).save('../../data/internal/result/segformer/k_'+str(i+1)+'/pred/'+file_path[0]+'.png')\n",
    "            test.set_description(\n",
    "                f\"val_Step: {count+1} dice_sore : {val_running_loss/count:.4f}\")\n",
    "    df.to_csv('../../data/internal/result/segformer/segformer_'+str(i+1)+'_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
