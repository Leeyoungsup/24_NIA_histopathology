{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from glob import glob\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp  # Mixed Precision Training\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\", 5)\n",
    "print(f\"Device:\\t\\t{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'image_size': 1024,\n",
    "          'lr': 5e-5,\n",
    "          'batch_size': 4,  # 배치 사이즈를 줄여 메모리 부담을 줄임\n",
    "          'epochs': 1000,\n",
    "          'n_classes': None,\n",
    "          'data_path': '../../data/origin_type/tar/**/**/',\n",
    "          'image_count': 5000,\n",
    "          'gradient_accumulation_steps': 2,  # Gradient Accumulation 설정\n",
    "          }\n",
    "tf = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self, parmas, L_image, RGB_image):\n",
    "\n",
    "        self.L_images = L_image\n",
    "        self.args = parmas\n",
    "        self.RGB_image = RGB_image\n",
    "\n",
    "    def trans(self, L_image,RGB_image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            L_image = transform(L_image)\n",
    "            RGB_image=transform(RGB_image)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            L_image = transform(L_image)\n",
    "            RGB_image=transform(RGB_image)\n",
    "\n",
    "        return L_image,RGB_image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        L_image = self.L_images[index]\n",
    "        RGB_image = self.RGB_image[index]\n",
    "        L_image ,RGB_image = self.trans(L_image,RGB_image)\n",
    "        return RGB_image,L_image \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.L_images)\n",
    "\n",
    "image_list = glob(params['data_path']+'*.jpeg')\n",
    "RGB_image_list=[f.replace('/tar', '/nor') for f in image_list]\n",
    "L_images=torch.zeros((len(image_list),3,params['image_size'],params['image_size']))\n",
    "RGB_images=torch.zeros((len(image_list),3,params['image_size'],params['image_size']))\n",
    "for i in tqdm(range(len(image_list))):\n",
    "    image=Image.open(image_list[i])\n",
    "    L_images[i]=tf(image.convert('L').convert('RGB').resize((params['image_size'], params['image_size'])))*2-1\n",
    "    image=Image.open(RGB_image_list[i])\n",
    "    RGB_images[i]=tf(image.convert('RGB').resize((params['image_size'], params['image_size'])))*2-1\n",
    "train_dataset = CustomDataset(params, L_images, RGB_images)\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, batch_size=params['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 아키텍처의 다운 샘플링(Down Sampling) 모듈\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        # 너비와 높이가 2배씩 감소\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# U-Net 아키텍처의 업 샘플링(Up Sampling) 모듈: Skip Connection 입력 사용\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        # ConvTranspose2d 대신 Upsample과 Conv2d 사용\n",
    "        layers = [nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                  nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                  nn.InstanceNorm2d(out_channels),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)  # 채널 레벨에서 합치기\n",
    "        return x\n",
    "\n",
    "\n",
    "# U-Net 생성자(Generator) 아키텍처\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)  # 출력: [64 x 512 x 512]\n",
    "        self.down2 = UNetDown(64, 128)                           # 출력: [128 x 256 x 256]\n",
    "        self.down3 = UNetDown(128, 256)                          # 출력: [256 x 128 x 128]\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)             # 출력: [512 x 64 x 64]\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)             # 출력: [512 x 32 x 32]\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)             # 출력: [512 x 16 x 16]\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)             # 출력: [512 x 8 x 8]\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)  # 출력: [512 x 4 x 4]\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)                 # 출력: [1024 x 8 x 8]\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)                # 출력: [1024 x 16 x 16]\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)                # 출력: [1024 x 32 x 32]\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)                # 출력: [1024 x 64 x 64]\n",
    "        self.up5 = UNetUp(1024, 256)                             # 출력: [512 x 128 x 128]\n",
    "        self.up6 = UNetUp(512, 128)                              # 출력: [256 x 256 x 256]\n",
    "        self.up7 = UNetUp(256, 64)                               # 출력: [128 x 512 x 512]\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),  # 출력: [128 x 1024 x 1024]\n",
    "            nn.Conv2d(128, out_channels, kernel_size=3, stride=1, padding=1),  # 출력: [3 x 1024 x 1024]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)\n",
    "\n",
    "\n",
    "# U-Net 판별자(Discriminator) 아키텍처\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_channels, out_channels, normalization=True):\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False),  # 출력: [64 x 512 x 512]\n",
    "            *discriminator_block(64, 128),                                  # 출력: [128 x 256 x 256]\n",
    "            *discriminator_block(128, 256),                                 # 출력: [256 x 128 x 128]\n",
    "            *discriminator_block(256, 512),                                 # 출력: [512 x 64 x 64]\n",
    "            *discriminator_block(512, 512),                                 # 출력: [512 x 32 x 32]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1, bias=False)         # 출력: [1 x 32 x 32]\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = amp.GradScaler()\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# 생성자(generator)와 판별자(discriminator) 초기화\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# 가중치(weights) 초기화\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# 손실 함수(loss function)\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "criterion_GAN.to(device)\n",
    "criterion_pixelwise.to(device)\n",
    "\n",
    "# 학습률(learning rate) 설정\n",
    "lr = 2e-5\n",
    "\n",
    "# 생성자와 판별자를 위한 최적화 함수\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# summary(generator, input_size=(params['batch_size'], 3, params['image_size'], params['image_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lambda_pixel = 50\n",
    "lambda1=1.01\n",
    "for epoch in range(params['epochs']):\n",
    "    total_loss_D = 0\n",
    "    total_loss_pixel = 0\n",
    "    total_loss_GAN = 0\n",
    "    steps = 0\n",
    "\n",
    "    optimizer_G.zero_grad()\n",
    "    optimizer_D.zero_grad()\n",
    "    \n",
    "    with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for i, (RGB_image, L_image) in enumerate(tqdmDataLoader):\n",
    "            # 모델의 입력 데이터 로드\n",
    "            real_A = L_image.to(device)  # Grayscale\n",
    "            real_B = RGB_image.to(device)  # Color\n",
    "\n",
    "            real = torch.FloatTensor(real_A.size(0), 1, 32, 32).fill_(0.9).to(device)  # 진짜\n",
    "            fake = torch.FloatTensor(real_A.size(0), 1, 32, 32).fill_(0.0).to(device)  # 가짜\n",
    "\n",
    "            # Mixed Precision Training으로 Generator 학습\n",
    "            with amp.autocast():\n",
    "                fake_B = generator(real_A)\n",
    "                loss_GAN = criterion_GAN(discriminator(fake_B, real_A), real)\n",
    "                loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "                loss_G = loss_GAN + (lambda_pixel*lambda1**epoch) * loss_pixel\n",
    "\n",
    "            # Gradient Accumulation을 사용하여 매 N 스텝마다 가중치 업데이트\n",
    "            scaler.scale(loss_G).backward()\n",
    "\n",
    "            \n",
    "            scaler.step(optimizer_G)\n",
    "            scaler.update()\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Discriminator 학습\n",
    "            with amp.autocast():\n",
    "                loss_real = criterion_GAN(discriminator(real_B, real_A), real)\n",
    "                loss_fake = criterion_GAN(discriminator(fake_B.detach(), real_A), fake)\n",
    "                loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "            scaler.scale(loss_D).backward()\n",
    "\n",
    "            if (i + 1) % params['gradient_accumulation_steps'] == 0:\n",
    "                scaler.step(optimizer_D)\n",
    "                scaler.update()\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "            total_loss_D += loss_D.item()\n",
    "            total_loss_pixel += loss_pixel.item()\n",
    "            total_loss_GAN += loss_GAN.item()\n",
    "            steps += 1\n",
    "\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"D loss: \": total_loss_D / steps,\n",
    "                    \"G pixel loss: \": total_loss_pixel / steps,\n",
    "                    \"adv loss: \": total_loss_GAN / steps,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # 이미지 샘플 저장\n",
    "    imgs = next(iter(dataloader))\n",
    "    real_A = (imgs[1].to(device) + 1) / 2\n",
    "    real_B = (imgs[0].to(device) + 1) / 2\n",
    "    fake_B = (generator(real_A) + 1) / 2\n",
    "    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "    to_pil_image(img_sample[0]).save(f'../../result/colorization/pix2pix_r/{epoch}.png')\n",
    "\n",
    "    # 모델 저장\n",
    "    torch.save(generator.state_dict(), f\"../../model/colorization/pix2pix_r/Pix2Pix_Generator_for_Colorization_{epoch}.pt\")\n",
    "    torch.save(discriminator.state_dict(), f\"../../model/colorization/pix2pix_r/Pix2Pix_Discriminator_for_Colorization_{epoch}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
